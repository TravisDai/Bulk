{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Bulk The bulk-synchronous parallel (BSP) paradigm is an effective framework for parallel programs. Bulk is a modern interface for writing BSP programs in C++. Modern programming language features allow for the implementation of safe and generic parallel algorithms for shared-memory, distributed-memory, and hybrid systems. Our interface should appeal to BSP programmers who want to write fast, safe, clear, and portable parallel programs. About BSP In the bulk-synchronous parallel (BSP) programming model, communication between processors (or nodes, or cores) does not happen asynchronously. Instead, all communication is staged and resolved at fixed synchronization points . These synchronizations delimit so-called supersteps . This way of structuring parallel programs has a number of advantages, at the cost of limiting the application\u2019s freedom to overlap communication and computation. The resulting programs are structured , easy to understand and maintain, and their performance and correctness can be reasoned about. Data races are eliminated almost by construction, because of simple rules which can be enforced at runtime. Scalability is straightforward to obtain. Programs are written in a SPMD fashion. There are only two types of communication mechanisms , message passing and named communication (through distributed variables) . This makes BSP based libraries economic: you can accomplish a lot with few building blocks. It has a gentle learning curve . It is easy to write correct BSP programs, while it is notoriously hard to write correct asynchronous parallel programs. Code examples Hello world! bulk :: thread :: environment env ; env . spawn ( env . available_processors (), []( auto & world ) { auto s = world . rank (); auto p = world . active_processors (); world . log ( \"Hello world from processor %d / %d!\" , s , p ); }); Distributed variables are the bread and butter of communication in Bulk. auto a = bulk :: var < int > ( world ); a ( world . next_rank ()) = s ; world . sync (); // ... a is now updated auto b = a ( world . next_rank ()). get (); world . sync (); // ... b.value() is now available Coarrays are convenient distributed arrays. auto xs = bulk :: coarray < int > ( world , 10 ); xs ( world . next_rank ())[ 3 ] = s ; Message passing can be used for more flexible communication. auto q = bulk :: queue < int , float > ( world ); for ( int t = 0 ; t < p ; ++ t ) { q ( t ). send ( s , 3.1415f ); // send (s, pi) to processor t } world . sync (); // messages are now available in q for ( auto [ tag , content ] : q ) { world . log ( \"%d got sent %d, %f \\n \" , s , tag , content ); } Building Bulk should work across all major Linux distributions. It requires an up-to-date compiler that supports C++17, e.g. GCC >= 7.0, or Clang >= 4.0. Backends Bulk supports a number of different backends , allowing the programs to run in parallel using: thread for multi-core systems using standard C++ <thread> threading support mpi for distributed environments using MPI There is also a special legacy backend available for the Epiphany coprocessor , which can be found in the epiphany branch. This branch has a modified version of Bulk to support portability between MPI, <thread> and the Epiphany coprocessor. See backends/epiphany/README.md for more details. Examples The examples in the examples directory work for every backend. To build them, do the following. The backends (e.g. thread , mpi ) are built optionally, just remove or add the option if you do not require them. mkdir build cd build cmake .. make thread mpi The examples will be compiled in the bin/{backend} directory, prepended with the backend name, i.e. to run the hello example with the thread backend: ./bin/thread/thread_hello Developing on top of Bulk The easiest way to get started using Bulk is to download the source code from GitHub . If you use Bulk in a project we suggest to add Bulk as a submodule: git submodule add https://www.github.com/jwbuurlage/bulk ext/bulk git submodule update --init If you use CMake for your project, adding Bulk as a dependency is straightforward. For this, you can use the bulk and bulk_[backend] targets. For example, if your CMake target is called your_program and it uses Bulk with the thread backend, you can use the following: add_subdirectory ( \"ext/bulk\" ) target_link_libraries ( your_program bulk_thread ) License Bulk is released under the MIT license, see LICENSE.md. Please Cite Us If you have used Bulk for a scientific publication, we would appreciate citations to the following paper: Buurlage JW., Bannink T., Bisseling R.H. (2018) Bulk: A Modern C++ Interface for Bulk-Synchronous Parallel Programs. In: Aldinucci M., Padovani L., Torquati M. (eds) Euro-Par 2018: Parallel Processing. Euro-Par 2018. Lecture Notes in Computer Science, vol 11014. Springer, Cham Applications using Bulk Article Code A projection-based partitioning method for distributed tomographic reconstruction . SIAM PP20. DOI Authors Bulk is developed at Centrum Wiskunde & Informatica (CWI) in Amsterdam by: Jan-Willem Buurlage (@jwbuurlage) Tom Bannink (@tombana) Also thanks to: Rob Bisseling Sarita de Berg (@SdeBerg) Contributing We welcome contributions! Please submit pull requests against the develop branch. For each PR: Describe the change in CHANGELOG.md New authors may add their name to the \u2018thanks to\u2019 section in README.md Format the code using clang-format , with the configuration found in the root of this project If you have any issues, questions, or remarks, then please open an issue on GitHub.","title":"Home"},{"location":"#bulk","text":"The bulk-synchronous parallel (BSP) paradigm is an effective framework for parallel programs. Bulk is a modern interface for writing BSP programs in C++. Modern programming language features allow for the implementation of safe and generic parallel algorithms for shared-memory, distributed-memory, and hybrid systems. Our interface should appeal to BSP programmers who want to write fast, safe, clear, and portable parallel programs.","title":"Bulk"},{"location":"#about-bsp","text":"In the bulk-synchronous parallel (BSP) programming model, communication between processors (or nodes, or cores) does not happen asynchronously. Instead, all communication is staged and resolved at fixed synchronization points . These synchronizations delimit so-called supersteps . This way of structuring parallel programs has a number of advantages, at the cost of limiting the application\u2019s freedom to overlap communication and computation. The resulting programs are structured , easy to understand and maintain, and their performance and correctness can be reasoned about. Data races are eliminated almost by construction, because of simple rules which can be enforced at runtime. Scalability is straightforward to obtain. Programs are written in a SPMD fashion. There are only two types of communication mechanisms , message passing and named communication (through distributed variables) . This makes BSP based libraries economic: you can accomplish a lot with few building blocks. It has a gentle learning curve . It is easy to write correct BSP programs, while it is notoriously hard to write correct asynchronous parallel programs.","title":"About BSP"},{"location":"#code-examples","text":"Hello world! bulk :: thread :: environment env ; env . spawn ( env . available_processors (), []( auto & world ) { auto s = world . rank (); auto p = world . active_processors (); world . log ( \"Hello world from processor %d / %d!\" , s , p ); }); Distributed variables are the bread and butter of communication in Bulk. auto a = bulk :: var < int > ( world ); a ( world . next_rank ()) = s ; world . sync (); // ... a is now updated auto b = a ( world . next_rank ()). get (); world . sync (); // ... b.value() is now available Coarrays are convenient distributed arrays. auto xs = bulk :: coarray < int > ( world , 10 ); xs ( world . next_rank ())[ 3 ] = s ; Message passing can be used for more flexible communication. auto q = bulk :: queue < int , float > ( world ); for ( int t = 0 ; t < p ; ++ t ) { q ( t ). send ( s , 3.1415f ); // send (s, pi) to processor t } world . sync (); // messages are now available in q for ( auto [ tag , content ] : q ) { world . log ( \"%d got sent %d, %f \\n \" , s , tag , content ); }","title":"Code examples"},{"location":"#building","text":"Bulk should work across all major Linux distributions. It requires an up-to-date compiler that supports C++17, e.g. GCC >= 7.0, or Clang >= 4.0.","title":"Building"},{"location":"#backends","text":"Bulk supports a number of different backends , allowing the programs to run in parallel using: thread for multi-core systems using standard C++ <thread> threading support mpi for distributed environments using MPI There is also a special legacy backend available for the Epiphany coprocessor , which can be found in the epiphany branch. This branch has a modified version of Bulk to support portability between MPI, <thread> and the Epiphany coprocessor. See backends/epiphany/README.md for more details.","title":"Backends"},{"location":"#examples","text":"The examples in the examples directory work for every backend. To build them, do the following. The backends (e.g. thread , mpi ) are built optionally, just remove or add the option if you do not require them. mkdir build cd build cmake .. make thread mpi The examples will be compiled in the bin/{backend} directory, prepended with the backend name, i.e. to run the hello example with the thread backend: ./bin/thread/thread_hello","title":"Examples"},{"location":"#developing-on-top-of-bulk","text":"The easiest way to get started using Bulk is to download the source code from GitHub . If you use Bulk in a project we suggest to add Bulk as a submodule: git submodule add https://www.github.com/jwbuurlage/bulk ext/bulk git submodule update --init If you use CMake for your project, adding Bulk as a dependency is straightforward. For this, you can use the bulk and bulk_[backend] targets. For example, if your CMake target is called your_program and it uses Bulk with the thread backend, you can use the following: add_subdirectory ( \"ext/bulk\" ) target_link_libraries ( your_program bulk_thread )","title":"Developing on top of Bulk"},{"location":"#license","text":"Bulk is released under the MIT license, see LICENSE.md.","title":"License"},{"location":"#please-cite-us","text":"If you have used Bulk for a scientific publication, we would appreciate citations to the following paper: Buurlage JW., Bannink T., Bisseling R.H. (2018) Bulk: A Modern C++ Interface for Bulk-Synchronous Parallel Programs. In: Aldinucci M., Padovani L., Torquati M. (eds) Euro-Par 2018: Parallel Processing. Euro-Par 2018. Lecture Notes in Computer Science, vol 11014. Springer, Cham","title":"Please Cite Us"},{"location":"#applications-using-bulk","text":"Article Code A projection-based partitioning method for distributed tomographic reconstruction . SIAM PP20. DOI","title":"Applications using Bulk"},{"location":"#authors","text":"Bulk is developed at Centrum Wiskunde & Informatica (CWI) in Amsterdam by: Jan-Willem Buurlage (@jwbuurlage) Tom Bannink (@tombana) Also thanks to: Rob Bisseling Sarita de Berg (@SdeBerg)","title":"Authors"},{"location":"#contributing","text":"We welcome contributions! Please submit pull requests against the develop branch. For each PR: Describe the change in CHANGELOG.md New authors may add their name to the \u2018thanks to\u2019 section in README.md Format the code using clang-format , with the configuration found in the root of this project If you have any issues, questions, or remarks, then please open an issue on GitHub.","title":"Contributing"},{"location":"CHANGELOG/","text":"Changelog All notable changes to Bulk will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning . Unreleased - YYYY-MM-DD Breaking changes bulk::index now represents indices as size_t Added Add clear_queues argument to world::sync , to optionally retain all messages from the previous superstep. Fixed Prevent bulk::sum and bulk::product from silently truncating values (@TimoMaarse, #11) 2.0.0 - 2020-03-27 Breaking changes Use size_t for indices in arrays and slices coarray::data now returns a T* instead of void* The iterator overload for coarray::put now requires an explicit offset Index type for D = 1 now wraps an int , instead of a singleton array Added Add foldl and foldl_each for coarrays. Add aliases for common folds: sum , product , min , and max , for values, variables and coarrays. Example : bulk::max(world, s + 1) == p . Add bulk::span for coarrays, allowing contiguous sequences as slice data sources in addition to std::vector . Add coarray::data to access the underlying (sequential) storage of coarrays. Allow externally managed data buffers for coarray . Add support for splitting a world into multiple parts, allowing subset syncs and communication. See also bulk::world::split . Add get_change to timer class (@SdeBerg, #10) Changed Use size_t for indices in arrays and slices coarray::data now returns a T* instead of void* The iterator overload for coarray::put now requires an explicit offset Index type for D = 1 now wraps an int , instead of a singleton array Fixed Fixed \u2018element owner\u2019 and \u2018local <-> global\u2019 computations for block partitionings with sizes that are not perfect divisors Fixed virtual qualifications on a number of internal functions 1.2.0 - 2018-11-02 Added Add CMake targets bulk and bulk_[backend] Add partitionings: cyclic, block, tree, rectangular, Cartesian Add parallel scientific computing examples and documentation Add partitioned_array Add Travis CI support Add world::log_once Fixed Remove duplicate sync from foldl Fix certain unittests relying on shared-memory access to success counter Fix coarray unittest relying on p > 2 Fix -Werror=format-security issue in world::log Changed Update default value for start_value of foldl to {} from 0 for non-numeric types Allow different type for accumulator of foldl 1.1.0 - 2018-10-10 Added Add a spinlock barrier bulk::thread::spinning_barrier to the thread backend, which is now used by default Add citation instruction to README 1.0.0 - 2018-02-27 Added Backend for the Epiphany coprocessor Fixed Let coarray::image::put take values by const reference Allow non-uniform local array sizes in coarray Require T to satisfy is_trivially_copyable for coarray<T> Add more type safety checks to (de-)serialization (De-)serialization in var and future now avoids redundant memory allocation and copying, by making the memory buffer objects non-owning 0.2.0 - 2017-08-09 Added Add support for std::string variables and queues Add support for array components in messages, i.e. queue<T[], U, V, ...> Deprecate processor_id in favor of rank , renamed {next,prev}_processor to {next, prev}_rank Fixed Fixed bug in MPI backend where messages could get truncated 0.1.0 - 2017-06-01 Initial release. A complete modern replacement for BSPlib.","title":"Release Notes"},{"location":"CHANGELOG/#changelog","text":"All notable changes to Bulk will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning .","title":"Changelog"},{"location":"CHANGELOG/#unreleased-yyyy-mm-dd","text":"","title":"Unreleased - YYYY-MM-DD"},{"location":"CHANGELOG/#breaking-changes","text":"bulk::index now represents indices as size_t","title":"Breaking changes"},{"location":"CHANGELOG/#added","text":"Add clear_queues argument to world::sync , to optionally retain all messages from the previous superstep.","title":"Added"},{"location":"CHANGELOG/#fixed","text":"Prevent bulk::sum and bulk::product from silently truncating values (@TimoMaarse, #11)","title":"Fixed"},{"location":"CHANGELOG/#200-2020-03-27","text":"","title":"2.0.0 - 2020-03-27"},{"location":"CHANGELOG/#breaking-changes_1","text":"Use size_t for indices in arrays and slices coarray::data now returns a T* instead of void* The iterator overload for coarray::put now requires an explicit offset Index type for D = 1 now wraps an int , instead of a singleton array","title":"Breaking changes"},{"location":"CHANGELOG/#added_1","text":"Add foldl and foldl_each for coarrays. Add aliases for common folds: sum , product , min , and max , for values, variables and coarrays. Example : bulk::max(world, s + 1) == p . Add bulk::span for coarrays, allowing contiguous sequences as slice data sources in addition to std::vector . Add coarray::data to access the underlying (sequential) storage of coarrays. Allow externally managed data buffers for coarray . Add support for splitting a world into multiple parts, allowing subset syncs and communication. See also bulk::world::split . Add get_change to timer class (@SdeBerg, #10)","title":"Added"},{"location":"CHANGELOG/#changed","text":"Use size_t for indices in arrays and slices coarray::data now returns a T* instead of void* The iterator overload for coarray::put now requires an explicit offset Index type for D = 1 now wraps an int , instead of a singleton array","title":"Changed"},{"location":"CHANGELOG/#fixed_1","text":"Fixed \u2018element owner\u2019 and \u2018local <-> global\u2019 computations for block partitionings with sizes that are not perfect divisors Fixed virtual qualifications on a number of internal functions","title":"Fixed"},{"location":"CHANGELOG/#120-2018-11-02","text":"","title":"1.2.0 - 2018-11-02"},{"location":"CHANGELOG/#added_2","text":"Add CMake targets bulk and bulk_[backend] Add partitionings: cyclic, block, tree, rectangular, Cartesian Add parallel scientific computing examples and documentation Add partitioned_array Add Travis CI support Add world::log_once","title":"Added"},{"location":"CHANGELOG/#fixed_2","text":"Remove duplicate sync from foldl Fix certain unittests relying on shared-memory access to success counter Fix coarray unittest relying on p > 2 Fix -Werror=format-security issue in world::log","title":"Fixed"},{"location":"CHANGELOG/#changed_1","text":"Update default value for start_value of foldl to {} from 0 for non-numeric types Allow different type for accumulator of foldl","title":"Changed"},{"location":"CHANGELOG/#110-2018-10-10","text":"","title":"1.1.0 - 2018-10-10"},{"location":"CHANGELOG/#added_3","text":"Add a spinlock barrier bulk::thread::spinning_barrier to the thread backend, which is now used by default Add citation instruction to README","title":"Added"},{"location":"CHANGELOG/#100-2018-02-27","text":"","title":"1.0.0 - 2018-02-27"},{"location":"CHANGELOG/#added_4","text":"Backend for the Epiphany coprocessor","title":"Added"},{"location":"CHANGELOG/#fixed_3","text":"Let coarray::image::put take values by const reference Allow non-uniform local array sizes in coarray Require T to satisfy is_trivially_copyable for coarray<T> Add more type safety checks to (de-)serialization (De-)serialization in var and future now avoids redundant memory allocation and copying, by making the memory buffer objects non-owning","title":"Fixed"},{"location":"CHANGELOG/#020-2017-08-09","text":"","title":"0.2.0 - 2017-08-09"},{"location":"CHANGELOG/#added_5","text":"Add support for std::string variables and queues Add support for array components in messages, i.e. queue<T[], U, V, ...> Deprecate processor_id in favor of rank , renamed {next,prev}_processor to {next, prev}_rank","title":"Added"},{"location":"CHANGELOG/#fixed_4","text":"Fixed bug in MPI backend where messages could get truncated","title":"Fixed"},{"location":"CHANGELOG/#010-2017-06-01","text":"Initial release. A complete modern replacement for BSPlib.","title":"0.1.0 - 2017-06-01"},{"location":"bsp/","text":"BSP Model The bulk synchronous parallel (BSP) model was developed by Leslie Valiant in the 1980s. The BSP model is intended as a bridging model between parallel hardware and software. It is an elegant and simple model that has a small and easy to understand interface. The BSP model is defined on an abstract computer called a BSP computer. This computer has three important requirements. It has p p processors capable of computation and communication, i.e. it allows for local memory transactions. It has a network in place that allows the different processors to send and receive data. It has a mechanism that allows for the synchronisation of these processors, e.g. by means of a blocking barrier. A BSP program consists of a number of distinct blocks of computation and communication called supersteps . These steps are separated by a barrier synchronisation, and consist of a computation and a communication step. An important part of a BSP algorithm is the associated cost function. To this end we introduce two important concepts: namely an h h -relation, and a notion of the work done by a processor. Furthermore we introduce two parameters that define a BSP computer: g g and l l . An h h -relation is a superstep in which each processor sends or receives a maximum of h h words of data. We commonly denote with s s the id of a processor such that we can write for the h h -relation: h = \\max_s \\left\\{ \\max \\{ (h_s)_\\text{sent}, (h_s)_\\text{received} \\}~|~\\text{processors } s \\right\\} h = \\max_s \\left\\{ \\max \\{ (h_s)_\\text{sent}, (h_s)_\\text{received} \\}~|~\\text{processors } s \\right\\} Where h_s h_s denotes the number of words received or sent by processor s s . Similarly we define the work w w done in a superstep as the maximum number of flops, floating point operations, performed by all processors. Finally we define the latency l l of a superstep as the fixed constant overhead, used primarily to account for the barrier synchronisation. The values for g g and l l are platform-specific constants that are found emperically. The values for w w and h h are superstep specific and commonly obtained analytically. The total BSP cost associated to a BSP algorithm is: T = \\sum_{\\text{supersteps } i} (w_i + g \\cdot h_i + l) T = \\sum_{\\text{supersteps } i} (w_i + g \\cdot h_i + l) The BSP model has gained significant interest in the last couple of years. Most notably because Google has adopted the model and has developed some technologies based on BSP such as MapReduce and Pregel. The standard for BSP implementations is BSPlib . Modern implementations of the BSPlib include BSPonMPI, which simulates the BSP model on top of MPI, and MulticoreBSP, which provides a BSP implementation for shared-memory multi-core computers. For a more detailed introduction on the BSP model, as well as a large number of examples of BSP programs we refer to the introductory textbook on BSP and MPI by Rob Bisseling. A large number of algorithms have already been implemented using the BSP model. Some of them with their associated cost function are listed below: Problem BSP Complexity Matrix multiplication n^3/p + (n^2/p^{2/3}) \\cdot g + l n^3/p + (n^2/p^{2/3}) \\cdot g + l Sorting (n \\log n)/p + (n/p)\\cdot g + l (n \\log n)/p + (n/p)\\cdot g + l Fast Fourier Transform (n \\log n)/p + (n/p)\\cdot g + l (n \\log n)/p + (n/p)\\cdot g + l LU Decomposition n^3/p + (n^2/p^{1/2})\\cdot g + p^{1/2}\\cdot l n^3/p + (n^2/p^{1/2})\\cdot g + p^{1/2}\\cdot l Cholesky Factorisation n^3/p + (n^2/p^{1/2})\\cdot g + p^{1/2}\\cdot l n^3/p + (n^2/p^{1/2})\\cdot g + p^{1/2}\\cdot l Algebraic Path Problem (Shortest Paths) n^3/p + (n^2/p^{1/2})\\cdot g + p^{1/2}\\cdot l n^3/p + (n^2/p^{1/2})\\cdot g + p^{1/2}\\cdot l Triangular Solver n^2/p + n\\cdot g + p\\cdot l n^2/p + n\\cdot g + p\\cdot l String Edit Problem n^2/p + n\\cdot g + p\\cdot l n^2/p + n\\cdot g + p\\cdot l Dense Matrix-Vector Multiplication n^2/p + (n/p^{1/2})\\cdot g+l n^2/p + (n/p^{1/2})\\cdot g+l Sparse Matrix-Vector Multiplication (2D grid) n/p + (n/p)^{1/2}\\cdot g+l n/p + (n/p)^{1/2}\\cdot g+l Sparse Matrix-Vector Multiplication (3D grid) n/p + (n/p)^{2/3}\\cdot g+l n/p + (n/p)^{2/3}\\cdot g+l Sparse Matrix-Vector Multiplication (random) n/p + (n/p)\\cdot g+l n/p + (n/p)\\cdot g+l List Ranking n/p + (n/p)\\cdot g+(\\log p)\\cdot l n/p + (n/p)\\cdot g+(\\log p)\\cdot l (From: McColl 1998 \u201cFoundations of Time-Critical Scalable Computing\u201d)","title":"BSP model"},{"location":"bsp/#bsp-model","text":"The bulk synchronous parallel (BSP) model was developed by Leslie Valiant in the 1980s. The BSP model is intended as a bridging model between parallel hardware and software. It is an elegant and simple model that has a small and easy to understand interface. The BSP model is defined on an abstract computer called a BSP computer. This computer has three important requirements. It has p p processors capable of computation and communication, i.e. it allows for local memory transactions. It has a network in place that allows the different processors to send and receive data. It has a mechanism that allows for the synchronisation of these processors, e.g. by means of a blocking barrier. A BSP program consists of a number of distinct blocks of computation and communication called supersteps . These steps are separated by a barrier synchronisation, and consist of a computation and a communication step. An important part of a BSP algorithm is the associated cost function. To this end we introduce two important concepts: namely an h h -relation, and a notion of the work done by a processor. Furthermore we introduce two parameters that define a BSP computer: g g and l l . An h h -relation is a superstep in which each processor sends or receives a maximum of h h words of data. We commonly denote with s s the id of a processor such that we can write for the h h -relation: h = \\max_s \\left\\{ \\max \\{ (h_s)_\\text{sent}, (h_s)_\\text{received} \\}~|~\\text{processors } s \\right\\} h = \\max_s \\left\\{ \\max \\{ (h_s)_\\text{sent}, (h_s)_\\text{received} \\}~|~\\text{processors } s \\right\\} Where h_s h_s denotes the number of words received or sent by processor s s . Similarly we define the work w w done in a superstep as the maximum number of flops, floating point operations, performed by all processors. Finally we define the latency l l of a superstep as the fixed constant overhead, used primarily to account for the barrier synchronisation. The values for g g and l l are platform-specific constants that are found emperically. The values for w w and h h are superstep specific and commonly obtained analytically. The total BSP cost associated to a BSP algorithm is: T = \\sum_{\\text{supersteps } i} (w_i + g \\cdot h_i + l) T = \\sum_{\\text{supersteps } i} (w_i + g \\cdot h_i + l) The BSP model has gained significant interest in the last couple of years. Most notably because Google has adopted the model and has developed some technologies based on BSP such as MapReduce and Pregel. The standard for BSP implementations is BSPlib . Modern implementations of the BSPlib include BSPonMPI, which simulates the BSP model on top of MPI, and MulticoreBSP, which provides a BSP implementation for shared-memory multi-core computers. For a more detailed introduction on the BSP model, as well as a large number of examples of BSP programs we refer to the introductory textbook on BSP and MPI by Rob Bisseling. A large number of algorithms have already been implemented using the BSP model. Some of them with their associated cost function are listed below: Problem BSP Complexity Matrix multiplication n^3/p + (n^2/p^{2/3}) \\cdot g + l n^3/p + (n^2/p^{2/3}) \\cdot g + l Sorting (n \\log n)/p + (n/p)\\cdot g + l (n \\log n)/p + (n/p)\\cdot g + l Fast Fourier Transform (n \\log n)/p + (n/p)\\cdot g + l (n \\log n)/p + (n/p)\\cdot g + l LU Decomposition n^3/p + (n^2/p^{1/2})\\cdot g + p^{1/2}\\cdot l n^3/p + (n^2/p^{1/2})\\cdot g + p^{1/2}\\cdot l Cholesky Factorisation n^3/p + (n^2/p^{1/2})\\cdot g + p^{1/2}\\cdot l n^3/p + (n^2/p^{1/2})\\cdot g + p^{1/2}\\cdot l Algebraic Path Problem (Shortest Paths) n^3/p + (n^2/p^{1/2})\\cdot g + p^{1/2}\\cdot l n^3/p + (n^2/p^{1/2})\\cdot g + p^{1/2}\\cdot l Triangular Solver n^2/p + n\\cdot g + p\\cdot l n^2/p + n\\cdot g + p\\cdot l String Edit Problem n^2/p + n\\cdot g + p\\cdot l n^2/p + n\\cdot g + p\\cdot l Dense Matrix-Vector Multiplication n^2/p + (n/p^{1/2})\\cdot g+l n^2/p + (n/p^{1/2})\\cdot g+l Sparse Matrix-Vector Multiplication (2D grid) n/p + (n/p)^{1/2}\\cdot g+l n/p + (n/p)^{1/2}\\cdot g+l Sparse Matrix-Vector Multiplication (3D grid) n/p + (n/p)^{2/3}\\cdot g+l n/p + (n/p)^{2/3}\\cdot g+l Sparse Matrix-Vector Multiplication (random) n/p + (n/p)\\cdot g+l n/p + (n/p)\\cdot g+l List Ranking n/p + (n/p)\\cdot g+(\\log p)\\cdot l n/p + (n/p)\\cdot g+(\\log p)\\cdot l (From: McColl 1998 \u201cFoundations of Time-Critical Scalable Computing\u201d)","title":"BSP Model"},{"location":"bulk_vs_bsplib/","text":"Bulk vs BSPlib In this section we highlight some of the differences between Bulk and BSPlib. No global state In BSPlib, the SPMD section is delimited using bsp_begin() and bsp_end() calls. In Bulk, the SPMD section is passed as an argument to the environment::spawn function. // BSPlib #include <bsp.h> int main () { bsp_begin ( bsp_nprocs ()); int s = bsp_pid (); int p = bsp_nprocs (); printf ( \"Hello World from processor %d / %d\" , s , p ); bsp_end (); return 0 ; } // Bulk #include <bulk/bulk.hpp> #include <bulk/backends/mpi/mpi.hpp> int main () { bulk :: mpi :: environment env ; env . spawn ( env . available_processors (), []( auto & world ) { auto s = world . rank (); auto p = world . active_processors (); world . log ( \"Hello world from processor %d / %d!\" , s , p ); }); } When using BSPlib, all calls to the parallel system happen globally. In particular, calls such as bsp_sync() and bsp_push_reg(...) implicitly modify some global state. In Bulk, the state is captured in a world object. Besides the well-known reasons not to use global state, this allows for multi-layered parallelism. For example, one world could contain the nodes in an MPI cluster, while another world represents the multi-core system on such a node. Registration of variables Variables and message queues are registered and initialized upon creation, and deregistered when they go out of scope. This means that explicit registration calls are no longer necessary. Compare: // BSPlib int x = 0 ; bsp_push_reg ( & x , sizeof ( int )); bsp_sync (); ... bsp_pop_reg ( & x ); // Bulk auto x = bulk :: var < int > ( world ); Communication Communication using simple distributed variables is expressed more compactly in Bulk. Compare: // BSPlib int b = 3 ; bsp_put (( s + 1 ) % p , & b , & x , 0 , sizeof ( int )); int c = 0 ; bsp_get (( s + 1 ) % p , & x , 0 , & c , sizeof ( int )); bsp_sync (); // Bulk x ( world . next_rank ()) = 3 ; auto c = x ( world . next_rank ()). get (); world . sync (); Arrays Bulk treats distributed arrays differently from simple values. This is done using coarrays. Compare: // BSPlib int * xs = malloc ( 10 * sizeof ( int )); bsp_push_reg ( xs , 10 * sizeof ( int )); bsp_sync (); int ys [ 3 ] = { 1 , 2 , 3 }; bsp_put (( s + 1 ) % p , ys , xs , 2 , 3 * sizeof ( int )); int z = 5 ; bsp_put (( s + 1 ) % p , & z , xs , 0 , sizeof ( int )); bsp_sync (); ... bsp_pop_reg ( xs ); free ( xs ); // Bulk auto xs = bulk :: coarray < int > ( world , 10 ); xs ( world . next_rank ())[{ 2 , 5 }] = { 2 , 3 , 4 }; xs ( world . next_rank ())[ 0 ] = 5 ; world . sync (); Message passing In BSPlib, messages consist of a tag and a content . In Bulk, we don\u2019t force this message structure, but we do support it. Compare: // BSPlib int s = bsp_pid (); int p = bsp_nprocs (); int tagsize = sizeof ( int ); bsp_set_tagsize ( & tagsize ); bsp_sync (); int tag = 1 ; int payload = 42 + s ; bsp_send (( s + 1 ) % p , & tag , & payload , sizeof ( int )); bsp_sync (); int packets = 0 ; int accum_bytes = 0 ; bsp_qsize ( & packets , & accum_bytes ); int payload_in = 0 ; int payload_size = 0 ; int tag_in = 0 ; for ( int i = 0 ; i < packets ; ++ i ) { bsp_get_tag ( & payload_size , & tag_in ); bsp_move ( & payload_in , sizeof ( int )); printf ( \"payload: %i, tag: %i\" , payload_in , tag_in ); } // Bulk auto s = world . rank (); auto p = world . active_processors (); auto q = bulk :: queue < int , int > ( world ); q ( world . next_rank ()). send ( 1 , 42 + s ); world . sync (); for ( auto [ tag , content ] : queue ) { world . log ( \"payload: %i, tag: %i\" , content , tag ); } In addition, Bulk supports sending arbitrary data either using custom structs, or by composing messages on the fly. For example, to send a 3D tensor element with indices and its value, we can write: auto q = bulk :: queue < int , int , int , float > ( world ); q ( world . next_rank ()). send ( 1 , 2 , 3 , 4.0f ); world . sync (); for ( auto [ i , j , k , value ] : queue ) { world . log ( \"element: A(%i, %i, %i) = %f\" , i , j , k , value ); } Also, multiple queues can be constructed, which eliminates a common use case for using tags. Other The algorithmic skeletons in Bulk allow common patterns to be implemented in fewer lines compared to BSPlib. As an example, we show how to find the maximum element over all processors. auto maxs = bulk :: gather_all ( world , max ); max = * std :: max_element ( maxs . begin (), maxs . end ()); Compare this to the way this is done when using BSPlib: int * global_max = malloc ( sizeof ( int ) * bsp_nprocs ()); bsp_push_reg ( global_max , sizeof ( int ) * bsp_nprocs ()); for ( int t = 0 ; t < p ; ++ t ) { bsp_put ( t , & max , global_max , bsp_pid (), sizeof ( int )); } bsp_sync (); for ( int t = 0 ; t < p ; ++ t ) { if ( max < global_max [ t ]) { max = global_max [ t ]; } } bsp_pop_reg ( global_max ); free ( global_max );","title":"Bulk vs BSPlib"},{"location":"bulk_vs_bsplib/#bulk-vs-bsplib","text":"In this section we highlight some of the differences between Bulk and BSPlib.","title":"Bulk vs BSPlib"},{"location":"bulk_vs_bsplib/#no-global-state","text":"In BSPlib, the SPMD section is delimited using bsp_begin() and bsp_end() calls. In Bulk, the SPMD section is passed as an argument to the environment::spawn function. // BSPlib #include <bsp.h> int main () { bsp_begin ( bsp_nprocs ()); int s = bsp_pid (); int p = bsp_nprocs (); printf ( \"Hello World from processor %d / %d\" , s , p ); bsp_end (); return 0 ; } // Bulk #include <bulk/bulk.hpp> #include <bulk/backends/mpi/mpi.hpp> int main () { bulk :: mpi :: environment env ; env . spawn ( env . available_processors (), []( auto & world ) { auto s = world . rank (); auto p = world . active_processors (); world . log ( \"Hello world from processor %d / %d!\" , s , p ); }); } When using BSPlib, all calls to the parallel system happen globally. In particular, calls such as bsp_sync() and bsp_push_reg(...) implicitly modify some global state. In Bulk, the state is captured in a world object. Besides the well-known reasons not to use global state, this allows for multi-layered parallelism. For example, one world could contain the nodes in an MPI cluster, while another world represents the multi-core system on such a node.","title":"No global state"},{"location":"bulk_vs_bsplib/#registration-of-variables","text":"Variables and message queues are registered and initialized upon creation, and deregistered when they go out of scope. This means that explicit registration calls are no longer necessary. Compare: // BSPlib int x = 0 ; bsp_push_reg ( & x , sizeof ( int )); bsp_sync (); ... bsp_pop_reg ( & x ); // Bulk auto x = bulk :: var < int > ( world );","title":"Registration of variables"},{"location":"bulk_vs_bsplib/#communication","text":"Communication using simple distributed variables is expressed more compactly in Bulk. Compare: // BSPlib int b = 3 ; bsp_put (( s + 1 ) % p , & b , & x , 0 , sizeof ( int )); int c = 0 ; bsp_get (( s + 1 ) % p , & x , 0 , & c , sizeof ( int )); bsp_sync (); // Bulk x ( world . next_rank ()) = 3 ; auto c = x ( world . next_rank ()). get (); world . sync ();","title":"Communication"},{"location":"bulk_vs_bsplib/#arrays","text":"Bulk treats distributed arrays differently from simple values. This is done using coarrays. Compare: // BSPlib int * xs = malloc ( 10 * sizeof ( int )); bsp_push_reg ( xs , 10 * sizeof ( int )); bsp_sync (); int ys [ 3 ] = { 1 , 2 , 3 }; bsp_put (( s + 1 ) % p , ys , xs , 2 , 3 * sizeof ( int )); int z = 5 ; bsp_put (( s + 1 ) % p , & z , xs , 0 , sizeof ( int )); bsp_sync (); ... bsp_pop_reg ( xs ); free ( xs ); // Bulk auto xs = bulk :: coarray < int > ( world , 10 ); xs ( world . next_rank ())[{ 2 , 5 }] = { 2 , 3 , 4 }; xs ( world . next_rank ())[ 0 ] = 5 ; world . sync ();","title":"Arrays"},{"location":"bulk_vs_bsplib/#message-passing","text":"In BSPlib, messages consist of a tag and a content . In Bulk, we don\u2019t force this message structure, but we do support it. Compare: // BSPlib int s = bsp_pid (); int p = bsp_nprocs (); int tagsize = sizeof ( int ); bsp_set_tagsize ( & tagsize ); bsp_sync (); int tag = 1 ; int payload = 42 + s ; bsp_send (( s + 1 ) % p , & tag , & payload , sizeof ( int )); bsp_sync (); int packets = 0 ; int accum_bytes = 0 ; bsp_qsize ( & packets , & accum_bytes ); int payload_in = 0 ; int payload_size = 0 ; int tag_in = 0 ; for ( int i = 0 ; i < packets ; ++ i ) { bsp_get_tag ( & payload_size , & tag_in ); bsp_move ( & payload_in , sizeof ( int )); printf ( \"payload: %i, tag: %i\" , payload_in , tag_in ); } // Bulk auto s = world . rank (); auto p = world . active_processors (); auto q = bulk :: queue < int , int > ( world ); q ( world . next_rank ()). send ( 1 , 42 + s ); world . sync (); for ( auto [ tag , content ] : queue ) { world . log ( \"payload: %i, tag: %i\" , content , tag ); } In addition, Bulk supports sending arbitrary data either using custom structs, or by composing messages on the fly. For example, to send a 3D tensor element with indices and its value, we can write: auto q = bulk :: queue < int , int , int , float > ( world ); q ( world . next_rank ()). send ( 1 , 2 , 3 , 4.0f ); world . sync (); for ( auto [ i , j , k , value ] : queue ) { world . log ( \"element: A(%i, %i, %i) = %f\" , i , j , k , value ); } Also, multiple queues can be constructed, which eliminates a common use case for using tags.","title":"Message passing"},{"location":"bulk_vs_bsplib/#other","text":"The algorithmic skeletons in Bulk allow common patterns to be implemented in fewer lines compared to BSPlib. As an example, we show how to find the maximum element over all processors. auto maxs = bulk :: gather_all ( world , max ); max = * std :: max_element ( maxs . begin (), maxs . end ()); Compare this to the way this is done when using BSPlib: int * global_max = malloc ( sizeof ( int ) * bsp_nprocs ()); bsp_push_reg ( global_max , sizeof ( int ) * bsp_nprocs ()); for ( int t = 0 ; t < p ; ++ t ) { bsp_put ( t , & max , global_max , bsp_pid (), sizeof ( int )); } bsp_sync (); for ( int t = 0 ; t < p ; ++ t ) { if ( max < global_max [ t ]) { max = global_max [ t ]; } } bsp_pop_reg ( global_max ); free ( global_max );","title":"Other"},{"location":"coarrays/","text":"Coarrays Coarrays are a convenient way to store, and manipulate distributed arrays. These distributed arrays can be seen as distributed variables, whose images are a local array. With Bulk, we provide a coarray that is modeled after Coarray Fortran . auto xs = bulk :: coarray < int > ( world , 5 ); Here, we create a coarray with local size equal to 5 . The total number of elements in the coarray is therefore 5 * p . We use a constant size here, but this is not required as the local size is allowed to vary over the processors. Coarrays provide syntactic sugar to make manipulating distributed arrays as easy as possible. For example, we can write: xs ( 3 )[ 2 ] = 1 ; This writes the value 1 to the element with local index 2 on the processor with index 3 . The local image of an array is iterable, so we can write for example: int result = 0 ; for ( auto x : xs ) { result += x ; } to compute the local sum of the numbers in the coarray image. Slices Often, you need to deal with multiple elements of a coarray image at once. For this, Bulk supports for slices. For example, to write to a range of elements at once: auto xs = bulk :: coarray < int > ( world , 10 ); xs ( world . next_rank ())[{ 2 , 5 }] = { 2 , 3 , 4 }; Or to get a range of elements: auto xs = bulk :: coarray < int > ( world , 10 ); auto ys = xs ( world . next_rank ())[{ 2 , 5 }]. get (); world . sync (); // ys[0], ys[1], ys[2] are now available;","title":"Coarrays"},{"location":"coarrays/#coarrays","text":"Coarrays are a convenient way to store, and manipulate distributed arrays. These distributed arrays can be seen as distributed variables, whose images are a local array. With Bulk, we provide a coarray that is modeled after Coarray Fortran . auto xs = bulk :: coarray < int > ( world , 5 ); Here, we create a coarray with local size equal to 5 . The total number of elements in the coarray is therefore 5 * p . We use a constant size here, but this is not required as the local size is allowed to vary over the processors. Coarrays provide syntactic sugar to make manipulating distributed arrays as easy as possible. For example, we can write: xs ( 3 )[ 2 ] = 1 ; This writes the value 1 to the element with local index 2 on the processor with index 3 . The local image of an array is iterable, so we can write for example: int result = 0 ; for ( auto x : xs ) { result += x ; } to compute the local sum of the numbers in the coarray image.","title":"Coarrays"},{"location":"coarrays/#slices","text":"Often, you need to deal with multiple elements of a coarray image at once. For this, Bulk supports for slices. For example, to write to a range of elements at once: auto xs = bulk :: coarray < int > ( world , 10 ); xs ( world . next_rank ())[{ 2 , 5 }] = { 2 , 3 , 4 }; Or to get a range of elements: auto xs = bulk :: coarray < int > ( world , 10 ); auto ys = xs ( world . next_rank ())[{ 2 , 5 }]. get (); world . sync (); // ys[0], ys[1], ys[2] are now available;","title":"Slices"},{"location":"environment_world/","text":"Environment and world In the upcoming sections we will get you started with programming in Bulk. Two important concepts are that of an environment and a world. In these code examples, we will often use the short-hand s for the local processor id which is referred to as its rank , and p for the active number of processors , and we will not always define these variables explicitly. Parallel environments A program runs in some parallel environment. For example, this environment could be an MPI cluster, a many-core co-processor, or simply threads on a multi-core computer. This environment is accessed within the program through a bulk::environment object. This object is specialized for each backend , which is an implementation of the lower-level communication that reflects the actual environment. For example, to setup an environment on an MPI cluster, we would write: #include <bulk/bulk.hpp> #include <bulk/backends/mpi/mpi.hpp> int main () { bulk :: mpi :: environment env ; } For a list of default providers, consult the backends section of this documentation. This environment object contains information on the parallel system, for example we can request the number of processors that are available. We note that throughout this documentation (and in the library), processor is a general term for the entity that executes the SPMD section (more on this later) and communicates with other processors \u2013 this can be an MPI node, a core, or a thread, depending on the backend that is used, but they are all treated in the same manner. auto processor_count = env . available_processors (); This information can be used to spawn the program on the right amount of processors. Programs written in Bulk follow the SPMD (Single Program Multiple Data) paradigm. This means that each processor executes the same code, but has its own (local) data that it manipulates. In Bulk, the SPMD section is a function object . This can be a C++ lambda, a std::function , or a C function pointer. In this documentation we will use lambda functions for our examples. This function will run on each processor, and should take three arguments. The first, contains the world object, which we will describe in detail in the next section. The SPMD section is executed in the following way: env . spawn ( env . available_processors (), []( auto & world ) { auto s = world . rank (); auto p = world . available_processors (); world . log ( \"Hello world from processor %d / %d!\" , s , p ); } The spawn function takes two arguments. The first is the total number of processors to run the SPMD section on, here we simply use all the processors that are available. The second is the SPMD function itself, that is run on the given number of processors. The world of a processor Each processor can communicate to other processors using the world object of type bulk::world . The world object contains some information on the specifics of the SPMD section, such as the number of processors executing the section, and its identifier (as we have seen, these are also provided as arguments for programmer convenience). We can also obtain indices of the neighbouring processors: auto next = world . next_rank (); auto previous = world . prev_rank (); The next and previous processor can also be computed manually using: next = ( s + 1 ) % p ; previous = ( s + p - 1 ) % p ; However, we would suggest using the appropriate methods of world to increase readability. Another important mechanism exposed through the world object is the ability to perform a bulk synchronization , which is the cornerstone of programs written in BSP style: world . sync (); We will see the specific uses of bulk synchronization in the upcoming sections.","title":"Environment and world"},{"location":"environment_world/#environment-and-world","text":"In the upcoming sections we will get you started with programming in Bulk. Two important concepts are that of an environment and a world. In these code examples, we will often use the short-hand s for the local processor id which is referred to as its rank , and p for the active number of processors , and we will not always define these variables explicitly.","title":"Environment and world"},{"location":"environment_world/#parallel-environments","text":"A program runs in some parallel environment. For example, this environment could be an MPI cluster, a many-core co-processor, or simply threads on a multi-core computer. This environment is accessed within the program through a bulk::environment object. This object is specialized for each backend , which is an implementation of the lower-level communication that reflects the actual environment. For example, to setup an environment on an MPI cluster, we would write: #include <bulk/bulk.hpp> #include <bulk/backends/mpi/mpi.hpp> int main () { bulk :: mpi :: environment env ; } For a list of default providers, consult the backends section of this documentation. This environment object contains information on the parallel system, for example we can request the number of processors that are available. We note that throughout this documentation (and in the library), processor is a general term for the entity that executes the SPMD section (more on this later) and communicates with other processors \u2013 this can be an MPI node, a core, or a thread, depending on the backend that is used, but they are all treated in the same manner. auto processor_count = env . available_processors (); This information can be used to spawn the program on the right amount of processors. Programs written in Bulk follow the SPMD (Single Program Multiple Data) paradigm. This means that each processor executes the same code, but has its own (local) data that it manipulates. In Bulk, the SPMD section is a function object . This can be a C++ lambda, a std::function , or a C function pointer. In this documentation we will use lambda functions for our examples. This function will run on each processor, and should take three arguments. The first, contains the world object, which we will describe in detail in the next section. The SPMD section is executed in the following way: env . spawn ( env . available_processors (), []( auto & world ) { auto s = world . rank (); auto p = world . available_processors (); world . log ( \"Hello world from processor %d / %d!\" , s , p ); } The spawn function takes two arguments. The first is the total number of processors to run the SPMD section on, here we simply use all the processors that are available. The second is the SPMD function itself, that is run on the given number of processors.","title":"Parallel environments"},{"location":"environment_world/#the-world-of-a-processor","text":"Each processor can communicate to other processors using the world object of type bulk::world . The world object contains some information on the specifics of the SPMD section, such as the number of processors executing the section, and its identifier (as we have seen, these are also provided as arguments for programmer convenience). We can also obtain indices of the neighbouring processors: auto next = world . next_rank (); auto previous = world . prev_rank (); The next and previous processor can also be computed manually using: next = ( s + 1 ) % p ; previous = ( s + p - 1 ) % p ; However, we would suggest using the appropriate methods of world to increase readability. Another important mechanism exposed through the world object is the ability to perform a bulk synchronization , which is the cornerstone of programs written in BSP style: world . sync (); We will see the specific uses of bulk synchronization in the upcoming sections.","title":"The world of a processor"},{"location":"getting_started/","text":"The easiest way to get started using Bulk is to download the source code from GitHub . If you use Bulk in a project we suggest to add Bulk as a submodule, since it is in active development. Bulk requires an up-to-date compiler, that supports C++17, e.g. GCC >= 7.0, or Clang >= 4.0. Currently we only actively support Linux, but we avoid platform specific code in the library, so building on other platforms should be possible. Bulk supports a number of different backends , allowing the programs to run in parallel using: thread for multi-core systems using standard C++ <thread> threading support mpi for distributed environments using MPI. We suggest to use the OpenMPI implementation, since is the implementation we test against. The examples in the examples directory work for every backend. They are built separately for each backend. The backends (e.g. thread , mpi ) are built optionally, just remove or add the option if you do not require them. mkdir build cd build cmake .. make thread mpi The examples will be compiled in the bin/{backend} directory, prepended with the backend name, i.e. to run the hello example with the thread backend: ./bin/thread/thread_hello Using Bulk in a project The easiest way to get started using Bulk is to download the source code from GitHub . If you use Bulk in a project we suggest to add Bulk as a submodule: git submodule add https://www.github.com/jwbuurlage/bulk ext/bulk git submodule update --init If you use CMake for your project, adding Bulk as a dependency is straightforward. For this, you can use the bulk and bulk_[backend] targets. For example, if your CMake target is called your_program and it uses Bulk with the thread backend, you can use the following: add_subdirectory ( \"ext/bulk\" ) target_link_libraries ( your_program bulk_thread ) If you do not use CMake, you can use the Bulk interface as a header-only library. Simply add ext/bulk/include as an include directory. Depending on the backend, you may have to link against addition libraries. See the backend\u2019s documentation for more information.","title":"Getting started"},{"location":"getting_started/#using-bulk-in-a-project","text":"The easiest way to get started using Bulk is to download the source code from GitHub . If you use Bulk in a project we suggest to add Bulk as a submodule: git submodule add https://www.github.com/jwbuurlage/bulk ext/bulk git submodule update --init If you use CMake for your project, adding Bulk as a dependency is straightforward. For this, you can use the bulk and bulk_[backend] targets. For example, if your CMake target is called your_program and it uses Bulk with the thread backend, you can use the following: add_subdirectory ( \"ext/bulk\" ) target_link_libraries ( your_program bulk_thread ) If you do not use CMake, you can use the Bulk interface as a header-only library. Simply add ext/bulk/include as an include directory. Depending on the backend, you may have to link against addition libraries. See the backend\u2019s documentation for more information.","title":"Using Bulk in a project"},{"location":"message_passing/","text":"Message passing Another way to communicate between processors is by using message queues. These queues can be used to send and receive an arbitrary number of messages . Messages have an attached tag and some content . Messages can be put into message queues, which have images on each processor. This queue can then be iterated over. To create a queue, you write: auto queue = bulk :: queue < int , float > ( world ); This will create a queue that stores message with integer tags, and float content. For example, a message can correspond to a component of a vector of floats. To put a message into a remote queue, we use queue(pid).send : queue ( world . next_rank ()). send ( 1 , 1.0f ); queue ( world . next_rank ()). send ( 2 , 5.0f ); This will send two messages to the next logical processor, with tags 1 and 2 respectively, and with contents 1.0f and 5.0f. As with communication through variables, this mechanism is also bulk synchronous , which means that the remote queue will only have access to the messages in the next superstep. Warning Message queues, like variables, are identified by the order in which they are constructed. Make sure this order is the same on each processor. world . sync (); for ( auto [ tag , content ] : queue ) { world . log ( \"Received tag: %d and content %f\" , tag , content ); }; Warning Message queues get cleared when calling world::sync . The messages in a queue therefore correspond only to those sent in the previous superstep. You can disable the clearing of queues using the clear_queues parameter of world::sync . It is perfectly legal, and even encouraged, to make a separate queue for different types of messages. Each message queue has its own independent types. In addition, you are not limited to \u2018tag + content\u2019 type of messages, you can also send untagged data, or custom data such as index tuples, or even your own structs. For example: auto raw_queue = bulk :: queue < int > ( world ); raw_queue ( world . next_rank ()). send ( 1 ); raw_queue ( world . next_rank ()). send ( 2 ); raw_queue ( world . next_rank ()). send ( 123 ); auto tuple_queue = bulk :: queue < int , int , int > ( world ); tuple_queue ( world . next_rank ()). send ( 1 , 2 , 3 ); tuple_queue ( world . next_rank ()). send ( 4 , 5 , 6 ); world . sync (); // read queue for ( auto x : raw_queue ) { world . log ( \"the first queue received a message: %d\" , x ); } for ( auto [ i , j , k ] : tuple_queue ) { world . log ( \"the second queue received a tuple: (%d, %d, %d)\" , i , j , k ); } It is also possible to send arrays using queues, by having a message component of type T[] . Components of queues of type T[] , require a std::vector<T> as input to send . Similarly, when iterating through the queue the T[] component of the message will be represented by a std::vector<T> . auto q = bulk :: queue < int [], int > ( world ); q ( world . next_rank ()). send ({ 1 , 2 , 3 , 4 }, 1 ); world . sync (); for ( auto [ xs , y ] : q ) { // ... xs is of type std::vector<int> }","title":"Message passing"},{"location":"message_passing/#message-passing","text":"Another way to communicate between processors is by using message queues. These queues can be used to send and receive an arbitrary number of messages . Messages have an attached tag and some content . Messages can be put into message queues, which have images on each processor. This queue can then be iterated over. To create a queue, you write: auto queue = bulk :: queue < int , float > ( world ); This will create a queue that stores message with integer tags, and float content. For example, a message can correspond to a component of a vector of floats. To put a message into a remote queue, we use queue(pid).send : queue ( world . next_rank ()). send ( 1 , 1.0f ); queue ( world . next_rank ()). send ( 2 , 5.0f ); This will send two messages to the next logical processor, with tags 1 and 2 respectively, and with contents 1.0f and 5.0f. As with communication through variables, this mechanism is also bulk synchronous , which means that the remote queue will only have access to the messages in the next superstep. Warning Message queues, like variables, are identified by the order in which they are constructed. Make sure this order is the same on each processor. world . sync (); for ( auto [ tag , content ] : queue ) { world . log ( \"Received tag: %d and content %f\" , tag , content ); }; Warning Message queues get cleared when calling world::sync . The messages in a queue therefore correspond only to those sent in the previous superstep. You can disable the clearing of queues using the clear_queues parameter of world::sync . It is perfectly legal, and even encouraged, to make a separate queue for different types of messages. Each message queue has its own independent types. In addition, you are not limited to \u2018tag + content\u2019 type of messages, you can also send untagged data, or custom data such as index tuples, or even your own structs. For example: auto raw_queue = bulk :: queue < int > ( world ); raw_queue ( world . next_rank ()). send ( 1 ); raw_queue ( world . next_rank ()). send ( 2 ); raw_queue ( world . next_rank ()). send ( 123 ); auto tuple_queue = bulk :: queue < int , int , int > ( world ); tuple_queue ( world . next_rank ()). send ( 1 , 2 , 3 ); tuple_queue ( world . next_rank ()). send ( 4 , 5 , 6 ); world . sync (); // read queue for ( auto x : raw_queue ) { world . log ( \"the first queue received a message: %d\" , x ); } for ( auto [ i , j , k ] : tuple_queue ) { world . log ( \"the second queue received a tuple: (%d, %d, %d)\" , i , j , k ); } It is also possible to send arrays using queues, by having a message component of type T[] . Components of queues of type T[] , require a std::vector<T> as input to send . Similarly, when iterating through the queue the T[] component of the message will be represented by a std::vector<T> . auto q = bulk :: queue < int [], int > ( world ); q ( world . next_rank ()). send ({ 1 , 2 , 3 , 4 }, 1 ); world . sync (); for ( auto [ xs , y ] : q ) { // ... xs is of type std::vector<int> }","title":"Message passing"},{"location":"psc/","text":"Introduction Large scale problems are common in scientific computing. For example, in the simulation of physical systems typically the grid size is chosen as fine as computational considerations allow for, and linear systems containing millions of equations and unknowns are solved routinely. In this tutorial, we will discuss the implementation of a number of important parallel algorithms in scientific computing. The algorithms we consider are those explained in Parallel Scientific Computing by Rob Bisseling . Implementations of these algorithms in BSPlib can be found in BSPedupack . Our discussion is stand-alone, however the focus is on the implementation of these algorithms in Bulk, and not the derivation or detailed description of the algorithms. The result of our efforts will be a library that contains support for important objects in parallel scientific computing: Distributed vector Distributed matrix (dense and sparse) The algorithms we will implement are (in order): Computing the inner product of two vectors Sorting a vector Computing the LU decomposition of a matrix Our intentions are educational, we will strive to design simple implementations of the algorithm and will not optimize for specific hardware, or e.g. exploit hybrid systems. Although we do not develop the algorithms for maximum performance, we will make sure that all our algorithms exhibit good scaling behavior. In most source code examples, we assume that we have the following aliases: auto s = world . rank (); auto p = world . active_processors (); The full source code is available in examples/psc starting from Bulk version v1.2.0. Note that this code may be slightly different from what we discuss here, for clarity of presentation, or because it has been updated. the psc::vector class: the humble beginnings of our library Before we are ready to describe and implement our first BSP algorithm, we will first define a distributed vector object. Since we will require multiple data distributions for the various algorithms we will implement, we will not assume any fixed one. The design of the vector object should be clear from the following example: env . spawn ( env . available_processors (), [ & ]( bulk :: world & world ) { auto s = world . rank (); auto p = world . active_processors (); // we choose n = 1e6 auto size = 1000000 ; // as an example, we take a cyclic partitioning auto pi = bulk :: cyclic_partitioning < 1 > ({ size }, { p }); auto v = psc :: vector < int > ( world , pi ); auto w = psc :: vector < int > ( world , pi ); // ... fill with data // compute the inner product auto alpha = psc :: dot ( v , w ); // we output the result world . log ( \"%i\" , alpha ); }); A partitioning pi is associated to a distributed vector. In this case, the components of the vector are integers. A psc::vector is defined as follows. namespace psc { template < typename T > class vector { //... Everything we define will be in a parallel scientific computing ( psc ) namespace. We make a generic implementation, that works for any T . A constructor for the vector class can look something like the following. private : bulk :: world & world_ ; bulk :: partitioning < 1 >& partitioning_ ; bulk :: coarray < T > data_ ; public : vector ( bulk :: world & world , bulk :: partitioning < 1 >& partitioning ) : world_ ( world ), partitioning_ ( partitioning ), data_ ( world_ , partitioning . local_count ( world_ . rank ()), ( T ) 0 ) {} // ... A vector object has three member objects; a reference to the world, a reference to the partitioning, and a coarray object in which is stores the data. The local size of this coarray is the number of local elements that we have been assigned according to the partitioning; as obtained by local_count . With this constructor, all elements are default initialized to zero. For completeness, we add some accessors and utility functions to our vector class: auto size () const { return data_ . size (); } auto global_size () const { return partitioning_ . global_size (); } bulk :: world & world () const { return world_ ; } bulk :: partitioning < 1 >& partitioning () const { return partitioning_ ; } auto begin () { return data_ . begin (); } auto end () { return data_ . end (); } This allows us to query the size of the vector, to obtain the communication world in which it is defined, the partitioning, and to use it as a generic container. With the vector object in place, we are ready to define our first BSP algorithm; the inner product computation. psc::dot : the inner product of two vectors Computing the inner product is fairly straightforward. We assume that the vectors are partitioned in the same manner. The algorithm proceeds in two steps. First, every processor computes the inner product of its local elements. Second, the local results are shared with an all-to-all broadcast and then accumulated resulting in the global result. There are multiple ways to achieve this using Bulk, but an elegent way is to use bulk::foldl over a bulk::var , which works as follows. Consider a distributed variable x . We can interpret x as a virtual 1D array, where the index in the index into the array corresponds to a processor id. s s 0 0 1 1 2 2 \u2026 p - 1 p - 1 x x(0) x(1) x(2) \u2026 x(p - 1) A left fold over a 1D array takes some binary operation + , and computes the reduction of the array using this operation: auto foldl = (((( x ( 0 ) + x ( 1 )) + x ( 2 )) + ...) + x ( p - 1 )) For the inner product, we actually want to use the sum as the binary operation, but if T would be for example a bool , we can also use an AND operation & , and a left fold would check if all local values are set to true . With the definition of a left fold in place, we are ready to describe the inner product algorithm. template < typename T > T dot ( const vector < T >& x , const vector < T >& y ) { bulk :: var < T > result ( x . world ()); for ( auto i = 0u ; i < x . size (); ++ i ) { result += x [ i ] * y [ i ]; } auto alpha = bulk :: foldl ( result , []( auto & lhs , auto rhs ) { lhs += rhs ; }); return alpha ; } Our template function dot takes two vectors, x and y , as input. The local results are stored in a distributed variable result , defined in the same world as the vectors. By a local computation, we set the local image of result to the correct value. Next, we call foldl on the distributed array result , and as binary operation we use a anonymous function that adds a value to another one. Finally, we return the result of this computation. There are other ways in which we could have implemented this algorithm. The communication pattern used (an all-to-all broadcast followed by a reduction) is so common that has a special implementation in Bulk, but the communication could also have been done manually. In any case, the end result is a generic parallel inner product that works on a vector regardless of its element type T , its size, or the partitioning that is employed. psc::sort : sorting a vector Next, we consider an example that is much more involved \u2013 sorting a vector. While for the inner product algorithm we did not need to assume a specific partitioning (only that both vectors involved employed the same one), we will choose a specific partitioning for sorting: a block partitioning. The following example shows how the parallel sorting algorithm is called by a user. auto pi = bulk :: block_partitioning < 1 > ({ size }, { p }); auto v = psc :: vector < int > ( world , pi ); psc :: sort ( v ); The parallel sorting algorithm we will implement is a distributed version of regular sample sorting. When the algorithm terminates, the result is a coarray where each element has roughly the same number of elements, every local array is sorted, and all elements on processor s > t s > t are bigger than those on t t . The algorithm consists of the following steps. Sorting the local vector . First, the local elements are sorted using any sequential sorting algorithm. Choosing and communicating p samples . Next, we identify p p equidistant samples. These p p elements are broadcast to all other processors. Finding global splitters . Every processor now has p^2 p^2 samples, which are sorted sequentially. From these samples, p p global equidistant samples are taken. These global samples define the splitters . A processor s s is responsible for a chunk of elements S_s \\leq x < S_{s + 1} S_s \\leq x < S_{s + 1} , where S_i S_i are the splitters. Identify local blocks . For all the local elements, the responsible processor is identified. Because the local array is sorted, the elements that have to be sent to a fixed processor t t are a consecutive block of the local array. Communicate blocks . Every block is sent to the appropriate processor. Merge the (individually sorted) blocks . Each processor now has p p blocks, which are all individually sorted. Using a merge sort routine the resulting sorted block can be computed efficiently. These steps can be implemented in Bulk as follows. First, sorting the local vector can be done simply by using the sorting routine from the standard library. template < typename T > bulk :: coarray < T > sort ( vector < T >& x ) { // 1. Sorting the local vector std :: sort ( x . begin (), x . end ()); // ... Next, we identify and communicate the local samples. Here, sample is a sequential routine that simply returns a number equidistant samples from an array. Also, blocked_merge takes an array consisting of a number of blocks that are individually sorted, and an array containing the sizes of the blocks, and sorts them together. In this case, after communicating, each processor has p p sorted blocks that are all of size p p . // 2. Choosing and communicating samples auto local_samples = sample ( x , p ); auto samples = bulk :: coarray < T > ( world , p * p ); for ( int t = 0 ; t < p ; ++ t ) { samples ( t )[{ s * p , ( s + 1 ) * p }] = local_samples ; } world . sync (); blocked_merge ( samples , std :: vector < int > ( p , p )); Now each processor has the same p^2 p^2 sorted samples, and we use it to compute the splitters. We set the upper limit for the final processor to \\infty \\infty . // 3. Finding global splitters auto splitters = sample ( samples , p ); splitters . push_back ( std :: numeric_limits < T >:: max ()); Next we want to split our local data up into chunks that each go to a different processor. We can write a simple sequential routine that does this, for example: // 4. Identify local blocks int idx = 0 ; int previous = 0 ; std :: vector < int > block_sizes ( p ); for ( int t = 0 ; t < p ; ++ t ) { while ( idx < ( int ) x . size () && x [ idx ] < splitters [ t + 1 ]) { ++ idx ; } block_sizes [ t ] = idx - previous ; previous = idx ; } Now, block_sizes(t) is the size of the local chunk that is to be sent to processor t . Next, we communicate the chunks. // 5. Communicate chunks auto block_starts = std :: vector < int > ( p ); std :: partial_sum ( block_sizes . begin (), block_sizes . end () - 1 , block_starts . begin () + 1 ); auto q = bulk :: queue < T [] > ( world ); for ( int t = 0 ; t < p ; ++ t ) { q ( t ). send ( std :: vector ( x . begin () + block_starts [ t ], x . begin () + block_starts [ t ] + block_sizes [ t ])); } world . sync (); Next, we first compute some information about the blocks: their sizes, and where they start. We define a coarray with the appropriate local size, and copy the chunks in to it. Finally, we perform a blocked_merge on the received chunks, and return the resulting coarray. // 6. Merge the (individually sorted) blocks auto receive_sizes = std :: vector < int > ( p ); std :: transform ( q . begin (), q . end (), receive_sizes . begin (), []( const auto & chunk ) { return chunk . size (); }); auto receive_starts = std :: vector < int > ( p ); std :: partial_sum ( receive_sizes . begin (), receive_sizes . end () - 1 , receive_starts . begin () + 1 ); auto total = std :: accumulate ( receive_sizes . begin (), receive_sizes . end (), 0 ); auto ys = bulk :: coarray < T > ( world , total ); auto t = 0u ; for ( const auto & chunk : q ) { std :: copy ( chunk . begin (), chunk . end (), ys . begin () + receive_starts [ t ++ ]); } blocked_merge ( ys , receive_sizes ); return ys ; Instead of a coarray, we could also return a vector with an irregular block partitioning , where the resulting sizes of the local array define the partitioning. psc::matrix class: distributed dense matrix Our class for a distributed dense matrix is a lot like the class we defined for a distributed dense vector. The main difference is in the data, which is now two-dimensional instead of one-dimensional. We will also choose a associate a Cartesian partitioning, and assume our p p processors are virtually laid out in a grid of size N \\times M N \\times M . The head of the class looks as follows: namespace psc { template < typename T > class matrix { //... We construct the matrix as follows: private : bulk :: world & world_ ; bulk :: cartesian_partitioning < 2 , 2 >& partitioning_ ; bulk :: coarray < T > data_ ; public : matrix ( bulk :: world & world , bulk :: cartesian_partitioning < 2 , 2 >& partitioning , T value = 0 ) : world_ ( world ), partitioning_ ( partitioning ), data_ ( world_ , partitioning . local_count ( world_ . rank ()), value ) {} // ... Note the Cartesian partitioning which has a 2 dimensional data space as well as a 2 dimensional grid space. Next, we provide a limited number of member functions: T & at ( bulk :: index_type < 2 > index ); auto & partitioning (); auto & world (); auto rows () { return partitioning_ . global_size ()[ 0 ]; } auto cols () { return partitioning_ . global_size ()[ 1 ]; } Here, at provides a reference to the matrix element with local index index . We provide rows and cols function for convenience, which forward the information from the partitioning. psc::lu : parallel in-place LU decomposition Now that we have a distributed matrix class, we are ready to implement a parallel LU decomposition routine. The parallel LU algorithm consists of the following steps. Find pivot Find, and communicate local maximum with processor column . Find global maximum element . Communicate global maximum with processor row . Swap rows Communicate permutation update . Update permutation vector . Communicate rows r and k . Swap rows r and k . Matrix update Communicate pivot . Scale column . Communicate row k and column k . Update bottom right block . We will use a two-dimensional processor numbering, obtained as follows: auto [ s , t ] = psi . multi_rank ( world . rank ()). get (); Here, psi is a cartesian_partitioning . We will not discuss the majority of sequential computations here, but they are given in the full example source code. We will show the body of the loop over k , which ranges from 0 to n - 1 where n is the matrix size. We assume the local maximum in column k is at row r_s and has value local_max . Then communicating with the local processor column can be done as follows: // (1) Communicate maximum element with local processor column if ( psi . owner ( 1 , k ) == t ) { for ( auto u = 0 ; u < psi . grid ()[ 0 ]; ++ u ) { pivot_rows ( psi . rank ({ u , t }))[ s ] = r_s ; pivot_values ( psi . rank ({ u , t }))[ s ] = local_max ; } } Here, pivot_rows and pivot_cols are two coarrays that are used to store all the local values. From these local maximums, the global maximums are found, and these can be communicated to the local processor row as follows. Afterwards, all processors know the pivot row r (stored inside a bulk::var<int> ). // (3) Communicate global maximum with local processor row if ( psi . owner ( 1 , k ) == t ) { for ( auto v = 0 ; v < psi . grid ()[ 1 ]; ++ v ) { r ( psi . rank ({ s , v })) = pivot_rows [ best_index ]; } world . sync (); We now want to swap rows k and r . First, we want to make sure our bookkeeping is up to date, and swap the components \\pi_r \\pi_r and \\pi_k \\pi_k of the permutation vector \\pi \\pi . We assume \\pi \\pi is stored in a coarray that is distributed according to the distribution function of the 0\\text{-th} 0\\text{-th} axis. // (4) Communicate permutation update if ( psi . owner ( 0 , k ) == s ) { pi ( psi . rank ({ psi . owner ( 0 , r ), t }))[ psi . local ( 0 , r )] = pi [ psi . local ( 0 , k )]; } if ( psi . owner ( 0 , r ) == s ) { pi ( psi . rank ({ psi . owner ( 0 , k ), t }))[ psi . local ( 0 , k )] = pi [ psi . local ( 0 , r )]; } world . sync (); Now that the permutation bookkeeping is done, we swap rows r and k . For this, we have two coarrays, which use local indices, that act as a buffer. // (6) Communicate row k and r if ( psi . owner ( 0 , k ) == s ) { for ( auto j = 0 ; j < psi . local_size ( 1 , t ); ++ j ) { row_swap_k ( psi . rank ({ psi . owner ( 0 , r ), t }))[ j ] = mat . at ({ psi . local ( 0 , k ), j }); } } if ( psi . owner ( 0 , r ) == s ) { for ( auto j = 0 ; j < psi . local_size ( 1 , t ); ++ j ) { row_swap_r ( psi . rank ({ psi . owner ( 0 , k ), t }))[ j ] = mat . at ({ psi . local ( 0 , r ), j }); } } We then swap the rows locally. // (7) Locally swap row k and r if ( psi . owner ( 0 , k ) == s ) { for ( auto j = 0 ; j < psi . local_size ( 1 , t ); ++ j ) { mat . at ({ psi . local ( 0 , k ), j }) = row_swap_r [ j ]; } } if ( psi . owner ( 0 , r ) == s ) { for ( auto j = 0 ; j < psi . local_size ( 1 , t ); ++ j ) { mat . at ({ psi . local ( 0 , r ), j }) = row_swap_k [ j ]; } } After swapping, we know the pivot element. This element is used to scale column k , so the processor that owns it should communicate it to its processor column. // (8) Communicate pivot if ( psi . owner ({ k , k }) == world . rank ()) { for ( auto u = 0 ; u < psi . grid ()[ 0 ]; ++ u ) { pivot ( psi . rank ({ u , t })) = mat . at ( psi . local ({ k , k })); } } world . sync (); Next, the column is scaled. // (9) Scale column if ( psi . owner ( 1 , k ) == t ) { // local row index, local col size, loop for ( auto i = ls [ k ]; i < psi . local_size ( 0 , s ); ++ i ) { mat . at ({ i , psi . local ( 1 , k )}) /= pivot ; } } For the matrix update, the row k and column k , to the right of and below the diagonal, should be communicated to the processors that require it. This is done using horizontal and vertical communication. Here, col_k and row_k are two coarrays that act as a buffer. // (10) Horizontal, communicate column k if ( psi . owner ( 1 , k ) == t ) { for ( auto v = 0 ; v < psi . grid ()[ 1 ]; ++ v ) { auto target_rank = psi . rank ({ s , v }); for ( auto i = 0 ; i < psi . local_size ( 0 , s ); ++ i ) { col_k ( target_rank )[ i ] = mat . at ({ i , psi . local ( 1 , k )}); } } } world . sync (); // (10b) Vertical, communicate row k if ( psi . owner ( 0 , k ) == s ) { for ( auto u = 0 ; u < psi . grid ()[ 0 ]; ++ u ) { auto target_rank = psi . rank ({ u , t }); for ( auto j = 0 ; j < psi . local_size ( 1 , t ); ++ j ) { row_k ( target_rank )[ j ] = mat . at ({ psi . local ( 0 , k ), j }); } } } world . sync (); Finally, we finish our matrix update. // (11) Update bottom right block for ( auto i = ls [ k ]; i < psi . local_size ( 0 , s ); ++ i ) { for ( auto j = qs [ k ]; j < psi . local_size ( 1 , t ); ++ j ) { mat . at ({ i , j }) -= col_k [ i ] * row_k [ j ]; } }","title":"Parallel Scientific Computing"},{"location":"psc/#introduction","text":"Large scale problems are common in scientific computing. For example, in the simulation of physical systems typically the grid size is chosen as fine as computational considerations allow for, and linear systems containing millions of equations and unknowns are solved routinely. In this tutorial, we will discuss the implementation of a number of important parallel algorithms in scientific computing. The algorithms we consider are those explained in Parallel Scientific Computing by Rob Bisseling . Implementations of these algorithms in BSPlib can be found in BSPedupack . Our discussion is stand-alone, however the focus is on the implementation of these algorithms in Bulk, and not the derivation or detailed description of the algorithms. The result of our efforts will be a library that contains support for important objects in parallel scientific computing: Distributed vector Distributed matrix (dense and sparse) The algorithms we will implement are (in order): Computing the inner product of two vectors Sorting a vector Computing the LU decomposition of a matrix Our intentions are educational, we will strive to design simple implementations of the algorithm and will not optimize for specific hardware, or e.g. exploit hybrid systems. Although we do not develop the algorithms for maximum performance, we will make sure that all our algorithms exhibit good scaling behavior. In most source code examples, we assume that we have the following aliases: auto s = world . rank (); auto p = world . active_processors (); The full source code is available in examples/psc starting from Bulk version v1.2.0. Note that this code may be slightly different from what we discuss here, for clarity of presentation, or because it has been updated.","title":"Introduction"},{"location":"psc/#the-pscvector-class-the-humble-beginnings-of-our-library","text":"Before we are ready to describe and implement our first BSP algorithm, we will first define a distributed vector object. Since we will require multiple data distributions for the various algorithms we will implement, we will not assume any fixed one. The design of the vector object should be clear from the following example: env . spawn ( env . available_processors (), [ & ]( bulk :: world & world ) { auto s = world . rank (); auto p = world . active_processors (); // we choose n = 1e6 auto size = 1000000 ; // as an example, we take a cyclic partitioning auto pi = bulk :: cyclic_partitioning < 1 > ({ size }, { p }); auto v = psc :: vector < int > ( world , pi ); auto w = psc :: vector < int > ( world , pi ); // ... fill with data // compute the inner product auto alpha = psc :: dot ( v , w ); // we output the result world . log ( \"%i\" , alpha ); }); A partitioning pi is associated to a distributed vector. In this case, the components of the vector are integers. A psc::vector is defined as follows. namespace psc { template < typename T > class vector { //... Everything we define will be in a parallel scientific computing ( psc ) namespace. We make a generic implementation, that works for any T . A constructor for the vector class can look something like the following. private : bulk :: world & world_ ; bulk :: partitioning < 1 >& partitioning_ ; bulk :: coarray < T > data_ ; public : vector ( bulk :: world & world , bulk :: partitioning < 1 >& partitioning ) : world_ ( world ), partitioning_ ( partitioning ), data_ ( world_ , partitioning . local_count ( world_ . rank ()), ( T ) 0 ) {} // ... A vector object has three member objects; a reference to the world, a reference to the partitioning, and a coarray object in which is stores the data. The local size of this coarray is the number of local elements that we have been assigned according to the partitioning; as obtained by local_count . With this constructor, all elements are default initialized to zero. For completeness, we add some accessors and utility functions to our vector class: auto size () const { return data_ . size (); } auto global_size () const { return partitioning_ . global_size (); } bulk :: world & world () const { return world_ ; } bulk :: partitioning < 1 >& partitioning () const { return partitioning_ ; } auto begin () { return data_ . begin (); } auto end () { return data_ . end (); } This allows us to query the size of the vector, to obtain the communication world in which it is defined, the partitioning, and to use it as a generic container. With the vector object in place, we are ready to define our first BSP algorithm; the inner product computation.","title":"the psc::vector class: the humble beginnings of our library"},{"location":"psc/#pscdot-the-inner-product-of-two-vectors","text":"Computing the inner product is fairly straightforward. We assume that the vectors are partitioned in the same manner. The algorithm proceeds in two steps. First, every processor computes the inner product of its local elements. Second, the local results are shared with an all-to-all broadcast and then accumulated resulting in the global result. There are multiple ways to achieve this using Bulk, but an elegent way is to use bulk::foldl over a bulk::var , which works as follows. Consider a distributed variable x . We can interpret x as a virtual 1D array, where the index in the index into the array corresponds to a processor id. s s 0 0 1 1 2 2 \u2026 p - 1 p - 1 x x(0) x(1) x(2) \u2026 x(p - 1) A left fold over a 1D array takes some binary operation + , and computes the reduction of the array using this operation: auto foldl = (((( x ( 0 ) + x ( 1 )) + x ( 2 )) + ...) + x ( p - 1 )) For the inner product, we actually want to use the sum as the binary operation, but if T would be for example a bool , we can also use an AND operation & , and a left fold would check if all local values are set to true . With the definition of a left fold in place, we are ready to describe the inner product algorithm. template < typename T > T dot ( const vector < T >& x , const vector < T >& y ) { bulk :: var < T > result ( x . world ()); for ( auto i = 0u ; i < x . size (); ++ i ) { result += x [ i ] * y [ i ]; } auto alpha = bulk :: foldl ( result , []( auto & lhs , auto rhs ) { lhs += rhs ; }); return alpha ; } Our template function dot takes two vectors, x and y , as input. The local results are stored in a distributed variable result , defined in the same world as the vectors. By a local computation, we set the local image of result to the correct value. Next, we call foldl on the distributed array result , and as binary operation we use a anonymous function that adds a value to another one. Finally, we return the result of this computation. There are other ways in which we could have implemented this algorithm. The communication pattern used (an all-to-all broadcast followed by a reduction) is so common that has a special implementation in Bulk, but the communication could also have been done manually. In any case, the end result is a generic parallel inner product that works on a vector regardless of its element type T , its size, or the partitioning that is employed.","title":"psc::dot: the inner product of two vectors"},{"location":"psc/#pscsort-sorting-a-vector","text":"Next, we consider an example that is much more involved \u2013 sorting a vector. While for the inner product algorithm we did not need to assume a specific partitioning (only that both vectors involved employed the same one), we will choose a specific partitioning for sorting: a block partitioning. The following example shows how the parallel sorting algorithm is called by a user. auto pi = bulk :: block_partitioning < 1 > ({ size }, { p }); auto v = psc :: vector < int > ( world , pi ); psc :: sort ( v ); The parallel sorting algorithm we will implement is a distributed version of regular sample sorting. When the algorithm terminates, the result is a coarray where each element has roughly the same number of elements, every local array is sorted, and all elements on processor s > t s > t are bigger than those on t t . The algorithm consists of the following steps. Sorting the local vector . First, the local elements are sorted using any sequential sorting algorithm. Choosing and communicating p samples . Next, we identify p p equidistant samples. These p p elements are broadcast to all other processors. Finding global splitters . Every processor now has p^2 p^2 samples, which are sorted sequentially. From these samples, p p global equidistant samples are taken. These global samples define the splitters . A processor s s is responsible for a chunk of elements S_s \\leq x < S_{s + 1} S_s \\leq x < S_{s + 1} , where S_i S_i are the splitters. Identify local blocks . For all the local elements, the responsible processor is identified. Because the local array is sorted, the elements that have to be sent to a fixed processor t t are a consecutive block of the local array. Communicate blocks . Every block is sent to the appropriate processor. Merge the (individually sorted) blocks . Each processor now has p p blocks, which are all individually sorted. Using a merge sort routine the resulting sorted block can be computed efficiently. These steps can be implemented in Bulk as follows. First, sorting the local vector can be done simply by using the sorting routine from the standard library. template < typename T > bulk :: coarray < T > sort ( vector < T >& x ) { // 1. Sorting the local vector std :: sort ( x . begin (), x . end ()); // ... Next, we identify and communicate the local samples. Here, sample is a sequential routine that simply returns a number equidistant samples from an array. Also, blocked_merge takes an array consisting of a number of blocks that are individually sorted, and an array containing the sizes of the blocks, and sorts them together. In this case, after communicating, each processor has p p sorted blocks that are all of size p p . // 2. Choosing and communicating samples auto local_samples = sample ( x , p ); auto samples = bulk :: coarray < T > ( world , p * p ); for ( int t = 0 ; t < p ; ++ t ) { samples ( t )[{ s * p , ( s + 1 ) * p }] = local_samples ; } world . sync (); blocked_merge ( samples , std :: vector < int > ( p , p )); Now each processor has the same p^2 p^2 sorted samples, and we use it to compute the splitters. We set the upper limit for the final processor to \\infty \\infty . // 3. Finding global splitters auto splitters = sample ( samples , p ); splitters . push_back ( std :: numeric_limits < T >:: max ()); Next we want to split our local data up into chunks that each go to a different processor. We can write a simple sequential routine that does this, for example: // 4. Identify local blocks int idx = 0 ; int previous = 0 ; std :: vector < int > block_sizes ( p ); for ( int t = 0 ; t < p ; ++ t ) { while ( idx < ( int ) x . size () && x [ idx ] < splitters [ t + 1 ]) { ++ idx ; } block_sizes [ t ] = idx - previous ; previous = idx ; } Now, block_sizes(t) is the size of the local chunk that is to be sent to processor t . Next, we communicate the chunks. // 5. Communicate chunks auto block_starts = std :: vector < int > ( p ); std :: partial_sum ( block_sizes . begin (), block_sizes . end () - 1 , block_starts . begin () + 1 ); auto q = bulk :: queue < T [] > ( world ); for ( int t = 0 ; t < p ; ++ t ) { q ( t ). send ( std :: vector ( x . begin () + block_starts [ t ], x . begin () + block_starts [ t ] + block_sizes [ t ])); } world . sync (); Next, we first compute some information about the blocks: their sizes, and where they start. We define a coarray with the appropriate local size, and copy the chunks in to it. Finally, we perform a blocked_merge on the received chunks, and return the resulting coarray. // 6. Merge the (individually sorted) blocks auto receive_sizes = std :: vector < int > ( p ); std :: transform ( q . begin (), q . end (), receive_sizes . begin (), []( const auto & chunk ) { return chunk . size (); }); auto receive_starts = std :: vector < int > ( p ); std :: partial_sum ( receive_sizes . begin (), receive_sizes . end () - 1 , receive_starts . begin () + 1 ); auto total = std :: accumulate ( receive_sizes . begin (), receive_sizes . end (), 0 ); auto ys = bulk :: coarray < T > ( world , total ); auto t = 0u ; for ( const auto & chunk : q ) { std :: copy ( chunk . begin (), chunk . end (), ys . begin () + receive_starts [ t ++ ]); } blocked_merge ( ys , receive_sizes ); return ys ; Instead of a coarray, we could also return a vector with an irregular block partitioning , where the resulting sizes of the local array define the partitioning.","title":"psc::sort: sorting a vector"},{"location":"psc/#pscmatrix-class-distributed-dense-matrix","text":"Our class for a distributed dense matrix is a lot like the class we defined for a distributed dense vector. The main difference is in the data, which is now two-dimensional instead of one-dimensional. We will also choose a associate a Cartesian partitioning, and assume our p p processors are virtually laid out in a grid of size N \\times M N \\times M . The head of the class looks as follows: namespace psc { template < typename T > class matrix { //... We construct the matrix as follows: private : bulk :: world & world_ ; bulk :: cartesian_partitioning < 2 , 2 >& partitioning_ ; bulk :: coarray < T > data_ ; public : matrix ( bulk :: world & world , bulk :: cartesian_partitioning < 2 , 2 >& partitioning , T value = 0 ) : world_ ( world ), partitioning_ ( partitioning ), data_ ( world_ , partitioning . local_count ( world_ . rank ()), value ) {} // ... Note the Cartesian partitioning which has a 2 dimensional data space as well as a 2 dimensional grid space. Next, we provide a limited number of member functions: T & at ( bulk :: index_type < 2 > index ); auto & partitioning (); auto & world (); auto rows () { return partitioning_ . global_size ()[ 0 ]; } auto cols () { return partitioning_ . global_size ()[ 1 ]; } Here, at provides a reference to the matrix element with local index index . We provide rows and cols function for convenience, which forward the information from the partitioning.","title":"psc::matrix class: distributed dense matrix"},{"location":"psc/#psclu-parallel-in-place-lu-decomposition","text":"Now that we have a distributed matrix class, we are ready to implement a parallel LU decomposition routine. The parallel LU algorithm consists of the following steps. Find pivot Find, and communicate local maximum with processor column . Find global maximum element . Communicate global maximum with processor row . Swap rows Communicate permutation update . Update permutation vector . Communicate rows r and k . Swap rows r and k . Matrix update Communicate pivot . Scale column . Communicate row k and column k . Update bottom right block . We will use a two-dimensional processor numbering, obtained as follows: auto [ s , t ] = psi . multi_rank ( world . rank ()). get (); Here, psi is a cartesian_partitioning . We will not discuss the majority of sequential computations here, but they are given in the full example source code. We will show the body of the loop over k , which ranges from 0 to n - 1 where n is the matrix size. We assume the local maximum in column k is at row r_s and has value local_max . Then communicating with the local processor column can be done as follows: // (1) Communicate maximum element with local processor column if ( psi . owner ( 1 , k ) == t ) { for ( auto u = 0 ; u < psi . grid ()[ 0 ]; ++ u ) { pivot_rows ( psi . rank ({ u , t }))[ s ] = r_s ; pivot_values ( psi . rank ({ u , t }))[ s ] = local_max ; } } Here, pivot_rows and pivot_cols are two coarrays that are used to store all the local values. From these local maximums, the global maximums are found, and these can be communicated to the local processor row as follows. Afterwards, all processors know the pivot row r (stored inside a bulk::var<int> ). // (3) Communicate global maximum with local processor row if ( psi . owner ( 1 , k ) == t ) { for ( auto v = 0 ; v < psi . grid ()[ 1 ]; ++ v ) { r ( psi . rank ({ s , v })) = pivot_rows [ best_index ]; } world . sync (); We now want to swap rows k and r . First, we want to make sure our bookkeeping is up to date, and swap the components \\pi_r \\pi_r and \\pi_k \\pi_k of the permutation vector \\pi \\pi . We assume \\pi \\pi is stored in a coarray that is distributed according to the distribution function of the 0\\text{-th} 0\\text{-th} axis. // (4) Communicate permutation update if ( psi . owner ( 0 , k ) == s ) { pi ( psi . rank ({ psi . owner ( 0 , r ), t }))[ psi . local ( 0 , r )] = pi [ psi . local ( 0 , k )]; } if ( psi . owner ( 0 , r ) == s ) { pi ( psi . rank ({ psi . owner ( 0 , k ), t }))[ psi . local ( 0 , k )] = pi [ psi . local ( 0 , r )]; } world . sync (); Now that the permutation bookkeeping is done, we swap rows r and k . For this, we have two coarrays, which use local indices, that act as a buffer. // (6) Communicate row k and r if ( psi . owner ( 0 , k ) == s ) { for ( auto j = 0 ; j < psi . local_size ( 1 , t ); ++ j ) { row_swap_k ( psi . rank ({ psi . owner ( 0 , r ), t }))[ j ] = mat . at ({ psi . local ( 0 , k ), j }); } } if ( psi . owner ( 0 , r ) == s ) { for ( auto j = 0 ; j < psi . local_size ( 1 , t ); ++ j ) { row_swap_r ( psi . rank ({ psi . owner ( 0 , k ), t }))[ j ] = mat . at ({ psi . local ( 0 , r ), j }); } } We then swap the rows locally. // (7) Locally swap row k and r if ( psi . owner ( 0 , k ) == s ) { for ( auto j = 0 ; j < psi . local_size ( 1 , t ); ++ j ) { mat . at ({ psi . local ( 0 , k ), j }) = row_swap_r [ j ]; } } if ( psi . owner ( 0 , r ) == s ) { for ( auto j = 0 ; j < psi . local_size ( 1 , t ); ++ j ) { mat . at ({ psi . local ( 0 , r ), j }) = row_swap_k [ j ]; } } After swapping, we know the pivot element. This element is used to scale column k , so the processor that owns it should communicate it to its processor column. // (8) Communicate pivot if ( psi . owner ({ k , k }) == world . rank ()) { for ( auto u = 0 ; u < psi . grid ()[ 0 ]; ++ u ) { pivot ( psi . rank ({ u , t })) = mat . at ( psi . local ({ k , k })); } } world . sync (); Next, the column is scaled. // (9) Scale column if ( psi . owner ( 1 , k ) == t ) { // local row index, local col size, loop for ( auto i = ls [ k ]; i < psi . local_size ( 0 , s ); ++ i ) { mat . at ({ i , psi . local ( 1 , k )}) /= pivot ; } } For the matrix update, the row k and column k , to the right of and below the diagonal, should be communicated to the processors that require it. This is done using horizontal and vertical communication. Here, col_k and row_k are two coarrays that act as a buffer. // (10) Horizontal, communicate column k if ( psi . owner ( 1 , k ) == t ) { for ( auto v = 0 ; v < psi . grid ()[ 1 ]; ++ v ) { auto target_rank = psi . rank ({ s , v }); for ( auto i = 0 ; i < psi . local_size ( 0 , s ); ++ i ) { col_k ( target_rank )[ i ] = mat . at ({ i , psi . local ( 1 , k )}); } } } world . sync (); // (10b) Vertical, communicate row k if ( psi . owner ( 0 , k ) == s ) { for ( auto u = 0 ; u < psi . grid ()[ 0 ]; ++ u ) { auto target_rank = psi . rank ({ u , t }); for ( auto j = 0 ; j < psi . local_size ( 1 , t ); ++ j ) { row_k ( target_rank )[ j ] = mat . at ({ psi . local ( 0 , k ), j }); } } } world . sync (); Finally, we finish our matrix update. // (11) Update bottom right block for ( auto i = ls [ k ]; i < psi . local_size ( 0 , s ); ++ i ) { for ( auto j = qs [ k ]; j < psi . local_size ( 1 , t ); ++ j ) { mat . at ({ i , j }) -= col_k [ i ] * row_k [ j ]; } }","title":"psc::lu: parallel in-place LU decomposition"},{"location":"tour/","text":"A tour of Bulk On this page we aim to highlight some of the main features of the library, and to give an impression of the overall syntax. Hello bulk::world ! We start out with the obligatory Hello World! in Bulk , and subsequently explain the code line-by-line. In this code we will use the MPI backend, but everything written here is completely general, and guaranteed to work on top of any conforming Bulk backend. #include <bulk/bulk.hpp> #include <bulk/backends/mpi/mpi.hpp> int main () { bulk :: mpi :: environment env ; env . spawn ( env . available_processors (), []( auto & world ) { auto s = world . rank (); auto p = world . active_processors (); world . log ( \"Hello world from processor %d / %d!\" , s , p ); }); } On lines 1 and 2 we include the library, and the backend of our choosing (in our case MPI). On line 5, we initialize an environment , which sets up the parallel or distributed system. On line 6, we spawn the SPMD section of our program within the environment. The first argument denotes the number of processors that we want to run the section on, while the second argument provides a function-like object (here a C++ lambda function) that is executed on the requested number of processors. This function obtains a bulk::world object, which it can use to communicate with other processors. For convenience we suggest to alias the processor identifier (or rank) to s and the total number of processes that are spawned to p . As shown in the example, these can be obtained from world using world.rank() and world.active_processors() respectively. Communication between processors Next, we look at some basic forms of communication between processors. The main way to talk to other processors, is by using variables. A variable is created as follows: auto x = bulk :: var < T > ( world ); Here, T is the type of the variable, for example an int . Values can be assigned to the (local) variable: x = 5 ; The reason to use such a distributed variable, is that a processor can write to a remote image of a variable. bulk :: put ( world . next_rank (), 4 , x ); // or the short-hand: x ( world . next_rank ()) = 4 ; This will overwrite the value of the variable x on the next logical processor (i.e. processor (s + 1) % p ) with 4 . We can obtain the value of a remote image using: auto y = bulk :: get ( world . next_rank (), x ); // or the short-hand: auto y = x ( world . next_rank ()). get (); Here, y is a bulk::future object. A future object does not immediately hold the remote value of x , but after a future call to world.sync() , we can extract the remote value out of y . world . sync (); auto x_next = y . value (); Coarrays Coarrays are a convenient way to store, and manipulate distributed data. We provide a coarray that is modeled after Coarray Fortran . Arrays are initialized and used as follows: auto xs = bulk :: coarray < int > ( world , s ); xs ( 3 )[ 2 ] = 1 ; Here, we create a coarray of varying local size (each processor holds s many elements). Next we write the value 1 to the element with local index 2 on processor with index 3 . Algorithmic skeletons Bulk comes equipped with a number of higher-level functions, also known as algorithmic skeletons . For example, say we want to compute the dot-product of two coarrays, then we write this as: auto xs = bulk :: coarray < int > ( world , s ); auto ys = bulk :: coarray < int > ( world , s ); // fill xs and ys with data auto result = bulk :: var < int > ( world ); for ( int i = 0 ; i < s ; ++ i ) { result . value () += xs [ i ] * ys [ i ]; } // reduce to find global dot product auto alpha = bulk :: foldl ( result , []( int & lhs , int rhs ) { lhs += rhs ; }); Here we first compute the local inner product, and finally use the higher-level function bulk::foldl result. Another example is finding a maximum element over all processors, here max is the maximum value found locally: auto maxs = bulk :: gather_all ( world , max ); max = * std :: max_element ( maxs . begin (), maxs . end ());","title":"Tour"},{"location":"tour/#a-tour-of-bulk","text":"On this page we aim to highlight some of the main features of the library, and to give an impression of the overall syntax.","title":"A tour of Bulk"},{"location":"tour/#hello-bulkworld","text":"We start out with the obligatory Hello World! in Bulk , and subsequently explain the code line-by-line. In this code we will use the MPI backend, but everything written here is completely general, and guaranteed to work on top of any conforming Bulk backend. #include <bulk/bulk.hpp> #include <bulk/backends/mpi/mpi.hpp> int main () { bulk :: mpi :: environment env ; env . spawn ( env . available_processors (), []( auto & world ) { auto s = world . rank (); auto p = world . active_processors (); world . log ( \"Hello world from processor %d / %d!\" , s , p ); }); } On lines 1 and 2 we include the library, and the backend of our choosing (in our case MPI). On line 5, we initialize an environment , which sets up the parallel or distributed system. On line 6, we spawn the SPMD section of our program within the environment. The first argument denotes the number of processors that we want to run the section on, while the second argument provides a function-like object (here a C++ lambda function) that is executed on the requested number of processors. This function obtains a bulk::world object, which it can use to communicate with other processors. For convenience we suggest to alias the processor identifier (or rank) to s and the total number of processes that are spawned to p . As shown in the example, these can be obtained from world using world.rank() and world.active_processors() respectively.","title":"Hello bulk::world!"},{"location":"tour/#communication-between-processors","text":"Next, we look at some basic forms of communication between processors. The main way to talk to other processors, is by using variables. A variable is created as follows: auto x = bulk :: var < T > ( world ); Here, T is the type of the variable, for example an int . Values can be assigned to the (local) variable: x = 5 ; The reason to use such a distributed variable, is that a processor can write to a remote image of a variable. bulk :: put ( world . next_rank (), 4 , x ); // or the short-hand: x ( world . next_rank ()) = 4 ; This will overwrite the value of the variable x on the next logical processor (i.e. processor (s + 1) % p ) with 4 . We can obtain the value of a remote image using: auto y = bulk :: get ( world . next_rank (), x ); // or the short-hand: auto y = x ( world . next_rank ()). get (); Here, y is a bulk::future object. A future object does not immediately hold the remote value of x , but after a future call to world.sync() , we can extract the remote value out of y . world . sync (); auto x_next = y . value ();","title":"Communication between processors"},{"location":"tour/#coarrays","text":"Coarrays are a convenient way to store, and manipulate distributed data. We provide a coarray that is modeled after Coarray Fortran . Arrays are initialized and used as follows: auto xs = bulk :: coarray < int > ( world , s ); xs ( 3 )[ 2 ] = 1 ; Here, we create a coarray of varying local size (each processor holds s many elements). Next we write the value 1 to the element with local index 2 on processor with index 3 .","title":"Coarrays"},{"location":"tour/#algorithmic-skeletons","text":"Bulk comes equipped with a number of higher-level functions, also known as algorithmic skeletons . For example, say we want to compute the dot-product of two coarrays, then we write this as: auto xs = bulk :: coarray < int > ( world , s ); auto ys = bulk :: coarray < int > ( world , s ); // fill xs and ys with data auto result = bulk :: var < int > ( world ); for ( int i = 0 ; i < s ; ++ i ) { result . value () += xs [ i ] * ys [ i ]; } // reduce to find global dot product auto alpha = bulk :: foldl ( result , []( int & lhs , int rhs ) { lhs += rhs ; }); Here we first compute the local inner product, and finally use the higher-level function bulk::foldl result. Another example is finding a maximum element over all processors, here max is the maximum value found locally: auto maxs = bulk :: gather_all ( world , max ); max = * std :: max_element ( maxs . begin (), maxs . end ());","title":"Algorithmic skeletons"},{"location":"variables/","text":"Variables Now that we are able to set up the environment, and gained some familiarity with the world object that can be used to communicate with other processors, we are ready to discuss communication between processors. The most fundamental way of communicating between processors is using distributed variables . The variables can be created as follows: auto x = bulk :: var < int > ( world ); Here we create a distributed variable that holds an integer. A distributed variable exists on every processor, but can have different values on different processors. These different local \u2018copies\u2019 are referred to as images of the variable. The variable lives within the world of the current SPMD section, and we explicitely write this by passing the world object as a parameter to the variable creation function. While x refers to the local image of a variable, it is identified with images on remote processors by the order in which variables are constructed (which is possible because of the SPMD nature of Bulk programs). This allows us to write to and read from remote images of a variable by simply passing x to communication functions. Bulk synchronous communication The main way to manipulate remote images of variables is using the communication primitives bulk::put and bulk::get for writing and reading respectively. For example, to write the value 1 to the remote image held by the next logical processor, we write: bulk::put(world.next_rank(), 1, x); To obtain the value of a remote image we write: auto y = bulk::get(world.next_rank(), x); Note Equivalently, we can use the short-hand syntax: x(world.next_rank()) = 1 for putting, and auto y = x(world.next_rank()).get() for getting. Initially, communication is only staged. This means that the values are not valid immediately after the execution of the communication primitives. Instead, they are available in the next \u2018section\u2019 of the program, known as a (BSP) superstep . Supersteps can be viewed as the section of the program within successive calls to world.sync() . This barrier synchronization asserts that all processors have reached that point of the program, and resolves all outstanding communication such as those staged by calls to put and get. After the barrier synchronization returns, all communication staged in the previous superstep is guaranteed to have occurred. In case of put , the remote image now contains the value written to it (assuming that the local processor is the only one who wrote to that specific remote image). For read requests using get , it is slightly more complicated. The type of y is a bulk::future<T> . A future object is a placeholder, that will contain the correct value in the next superstep. This value can be obtained after the synchronization using: auto value = y.value(); This way of communicating is particularly useful when dealing with simple data objects. If instead we deal with distributed array-like objects, we recommend using coarrays , which are introduced in the next section.","title":"Distributed variables"},{"location":"variables/#variables","text":"Now that we are able to set up the environment, and gained some familiarity with the world object that can be used to communicate with other processors, we are ready to discuss communication between processors. The most fundamental way of communicating between processors is using distributed variables . The variables can be created as follows: auto x = bulk :: var < int > ( world ); Here we create a distributed variable that holds an integer. A distributed variable exists on every processor, but can have different values on different processors. These different local \u2018copies\u2019 are referred to as images of the variable. The variable lives within the world of the current SPMD section, and we explicitely write this by passing the world object as a parameter to the variable creation function. While x refers to the local image of a variable, it is identified with images on remote processors by the order in which variables are constructed (which is possible because of the SPMD nature of Bulk programs). This allows us to write to and read from remote images of a variable by simply passing x to communication functions.","title":"Variables"},{"location":"variables/#bulk-synchronous-communication","text":"The main way to manipulate remote images of variables is using the communication primitives bulk::put and bulk::get for writing and reading respectively. For example, to write the value 1 to the remote image held by the next logical processor, we write: bulk::put(world.next_rank(), 1, x); To obtain the value of a remote image we write: auto y = bulk::get(world.next_rank(), x); Note Equivalently, we can use the short-hand syntax: x(world.next_rank()) = 1 for putting, and auto y = x(world.next_rank()).get() for getting. Initially, communication is only staged. This means that the values are not valid immediately after the execution of the communication primitives. Instead, they are available in the next \u2018section\u2019 of the program, known as a (BSP) superstep . Supersteps can be viewed as the section of the program within successive calls to world.sync() . This barrier synchronization asserts that all processors have reached that point of the program, and resolves all outstanding communication such as those staged by calls to put and get. After the barrier synchronization returns, all communication staged in the previous superstep is guaranteed to have occurred. In case of put , the remote image now contains the value written to it (assuming that the local processor is the only one who wrote to that specific remote image). For read requests using get , it is slightly more complicated. The type of y is a bulk::future<T> . A future object is a placeholder, that will contain the correct value in the next superstep. This value can be obtained after the synchronization using: auto value = y.value(); This way of communicating is particularly useful when dealing with simple data objects. If instead we deal with distributed array-like objects, we recommend using coarrays , which are introduced in the next section.","title":"Bulk synchronous communication"},{"location":"api/","text":"Bulk API reference System bulk::environment the central object that encapsulates the distributed system bulk::world an implementation of the low-level functions Distributed objects and communication bulk::var a distributed variable with an image for each processor bulk::future an object which encapsulates a value known in future supersteps bulk::coarray a distributed array with a local array image for each processor Message passing bulk::queue a container containing messages Algorithms bulk::foldl a left fold over a var bulk::gather_all gather results Partitionings bulk::partitioning index computations for data distributions Utility bulk::util::timer wall timer for benchmarking bulk::util::flatten flatten multi-indices bulk::util::unflatten unflatten mutli-indices","title":"Index"},{"location":"api/#bulk-api-reference","text":"System bulk::environment the central object that encapsulates the distributed system bulk::world an implementation of the low-level functions Distributed objects and communication bulk::var a distributed variable with an image for each processor bulk::future an object which encapsulates a value known in future supersteps bulk::coarray a distributed array with a local array image for each processor Message passing bulk::queue a container containing messages Algorithms bulk::foldl a left fold over a var bulk::gather_all gather results Partitionings bulk::partitioning index computations for data distributions Utility bulk::util::timer wall timer for benchmarking bulk::util::flatten flatten multi-indices bulk::util::unflatten unflatten mutli-indices","title":"Bulk API reference"},{"location":"api/coarray/","text":"bulk::coarray Defined in header <bulk/coarray.hpp> . template < typename T > class coarray ; Distributed array with easy element access, loosely based on the behaviour of Coarray Fortran . Usage Coarrays provide a convenient way to share data across processors. Instead of manually sending and receiving data elements, coarrays model distributed data as a 2-dimensional array, where the first dimension is over the processors, and the second dimension is over local 1-dimensional array indices. Template parameters T - the type of the value stored in the local image of the coarray. Member types value_type : the type of the image values (i.e. T ). Nested classes image representation of coarray image writer allows for modification for remote coarray image slice slices of coarrays slice_writer modify slices of coarrays Member functions (constructor) constructs the coarray (deconstructor) deconstructs the coarray Value access operator() obtain an image of the coarray operator[] access the local elements of the coarray World access world returns the world to which the coarray belongs Example auto xs = bulk :: coarray < int > ( world , 10 ); // set the 5th element on the 1st processor to 4 xs ( 1 )[ 5 ] = 4 ; // set the 3rd element on the local processor to 2 xs [ 3 ] = 2 ; // set a slice xs ( 1 )[{ 2 , 5 }] = { 1 , 2 , 3 }; // get a slice auto values = xs ( 2 )[{ 2 , 5 }]. get ();","title":"coarray"},{"location":"api/coarray/#bulkcoarray","text":"Defined in header <bulk/coarray.hpp> . template < typename T > class coarray ; Distributed array with easy element access, loosely based on the behaviour of Coarray Fortran .","title":"bulk::coarray"},{"location":"api/coarray/#usage","text":"Coarrays provide a convenient way to share data across processors. Instead of manually sending and receiving data elements, coarrays model distributed data as a 2-dimensional array, where the first dimension is over the processors, and the second dimension is over local 1-dimensional array indices.","title":"Usage"},{"location":"api/coarray/#template-parameters","text":"T - the type of the value stored in the local image of the coarray.","title":"Template parameters"},{"location":"api/coarray/#member-types","text":"value_type : the type of the image values (i.e. T ).","title":"Member types"},{"location":"api/coarray/#nested-classes","text":"image representation of coarray image writer allows for modification for remote coarray image slice slices of coarrays slice_writer modify slices of coarrays","title":"Nested classes"},{"location":"api/coarray/#member-functions","text":"(constructor) constructs the coarray (deconstructor) deconstructs the coarray Value access operator() obtain an image of the coarray operator[] access the local elements of the coarray World access world returns the world to which the coarray belongs","title":"Member functions"},{"location":"api/coarray/#example","text":"auto xs = bulk :: coarray < int > ( world , 10 ); // set the 5th element on the 1st processor to 4 xs ( 1 )[ 5 ] = 4 ; // set the 3rd element on the local processor to 2 xs [ 3 ] = 2 ; // set a slice xs ( 1 )[{ 2 , 5 }] = { 1 , 2 , 3 }; // get a slice auto values = xs ( 2 )[{ 2 , 5 }]. get ();","title":"Example"},{"location":"api/environment/","text":"bulk::environment Defined in header <bulk/environment.hpp> . class environment ; bulk::environment encodes the environment of a parallel layer, and provides information on the system. Member functions Initialization spawn spawns a spmd section on a given number of processors System information available_processors returns the number of available processors Example #include \"bulk/bulk.hpp\" #include \"set_backend.hpp\" int main () { environment env ; env . spawn ( env . available_processors (), []( bulk :: world & world ) { int s = world . processor_id (); int p = world . active_processors (); world . log ( \"Hello, world %d/%d\" , s , p ); }); return 0 ; }","title":"environment"},{"location":"api/environment/#bulkenvironment","text":"Defined in header <bulk/environment.hpp> . class environment ; bulk::environment encodes the environment of a parallel layer, and provides information on the system.","title":"bulk::environment"},{"location":"api/environment/#member-functions","text":"Initialization spawn spawns a spmd section on a given number of processors System information available_processors returns the number of available processors","title":"Member functions"},{"location":"api/environment/#example","text":"#include \"bulk/bulk.hpp\" #include \"set_backend.hpp\" int main () { environment env ; env . spawn ( env . available_processors (), []( bulk :: world & world ) { int s = world . processor_id (); int p = world . active_processors (); world . log ( \"Hello, world %d/%d\" , s , p ); }); return 0 ; }","title":"Example"},{"location":"api/flatten/","text":"bulk::util::flatten template < int D > int flatten ( index < D > volume , index < D > idxs ); Flatten a multi-index in a D-dimensional volume. Template parameters D - the dimension of the index space Parameters volume - the size of the volume idxs - the multi-index","title":"flatten"},{"location":"api/flatten/#bulkutilflatten","text":"template < int D > int flatten ( index < D > volume , index < D > idxs ); Flatten a multi-index in a D-dimensional volume.","title":"bulk::util::flatten"},{"location":"api/flatten/#template-parameters","text":"D - the dimension of the index space","title":"Template parameters"},{"location":"api/flatten/#parameters","text":"volume - the size of the volume idxs - the multi-index","title":"Parameters"},{"location":"api/foldl/","text":"bulk::foldl template < typename T , typename Func , typename S = T > S foldl ( var < T >& x , Func f , S start_value = {}) // (1) template < typename T , typename Func , typename S = T > S foldl ( coarray < T >& xs , Func f , S start_value = {}) // (2) Perform a left-fold over a distributed variable (1) or coarray (2). Template parameters T - the value type of the variable/array S - the value type of the accumulator Func - type of a binary function of the form (S&, T) -> void that modifies its first parameter, and is used in the fold Parameters x - the distributed variable xs - the coarray f - the folding function start_value - the (optional) initial value of the accumulator Complexity and cost Cost : (1) costof(f) * p + sizeof(T) * p * g + l (2) costof(f) * (p + X) + sizeof(T) * p * g + l bulk::foldl_each template < typename T , typename Func , typename S = T > std :: vector < T > foldl_each ( coarray < T >& xs , Func f , S start_value = {}) Perform a left-fold over each element of a coarray. Template parameters T - the value type of the variable/array S - the value type of the accumulator Func - type of a binary function of the form (S&, T) -> void that modifies its first parameter, and is used in the fold Parameters xs - the coarray. Required the same local size on each processors, or an empty array is returned. f - the folding function start_value - the (optional) initial value of the accumulator Result The i i th element of the result holds a left fold of f over the i i th element of each local image of the coarray xs . Complexity and cost Cost : costof(f) * p + sizeof(T) * xs.size() * p * g + l bulk::max template < typename T > T max ( bulk :: world & world , T t ); // (1) template < typename T > T max ( bulk :: var < T >& x ); // (2) template < typename T > T max ( bulk :: coarray < T >& xs ); // (3) Compute the global maximum of local values. Template parameters T - the value type of the value/variable/array Parameters t - the value x - the distributed variable xs - the coarray Complexity and cost Cost : (1), (2) p + sizeof(T) * p * g + l (3) xs.size() * (xs.size() * p + sizeof(T) * p * g) + l bulk::min template < typename T > T min ( bulk :: world & world , T t ); // (1) template < typename T > T min ( bulk :: var < T >& x ); // (2) template < typename T > T min ( bulk :: coarray < T >& xs ); // (3) Compute the global minimum of local values. Template parameters T - the value type of the value/variable/array Parameters t - the value x - the distributed variable xs - the coarray Complexity and cost Cost : (1), (2) p + sizeof(T) * p * g + l (3) xs.size() * (xs.size() * p + sizeof(T) * p * g) + l bulk::sum template < typename T > T sum ( bulk :: world & world , T t ); // (1) template < typename T > T sum ( bulk :: var < T >& x ); // (2) template < typename T > T sum ( bulk :: coarray < T >& xs ); // (3) Compute the global sum of local values. Template parameters T - the value type of the value/variable/array Parameters t - the value x - the distributed variable xs - the coarray Complexity and cost Cost : (1), (2) p + sizeof(T) * p * g + l (3) xs.size() * (xs.size() * p + sizeof(T) * p * g) + l bulk::product template < typename T > T product ( bulk :: world & world , T t ); // (1) template < typename T > T product ( bulk :: var < T >& x ); // (2) template < typename T > T product ( bulk :: coarray < T >& xs ); // (3) Compute the global product of local values. Template parameters T - the value type of the value/variable/array Parameters t - the value x - the distributed variable xs - the coarray Complexity and cost Cost : (1), (2) p + sizeof(T) * p * g + l (3) xs.size() * (xs.size() * p + sizeof(T) * p * g) + l","title":"foldl / max / sum / ..."},{"location":"api/foldl/#bulkfoldl","text":"template < typename T , typename Func , typename S = T > S foldl ( var < T >& x , Func f , S start_value = {}) // (1) template < typename T , typename Func , typename S = T > S foldl ( coarray < T >& xs , Func f , S start_value = {}) // (2) Perform a left-fold over a distributed variable (1) or coarray (2).","title":"bulk::foldl"},{"location":"api/foldl/#template-parameters","text":"T - the value type of the variable/array S - the value type of the accumulator Func - type of a binary function of the form (S&, T) -> void that modifies its first parameter, and is used in the fold","title":"Template parameters"},{"location":"api/foldl/#parameters","text":"x - the distributed variable xs - the coarray f - the folding function start_value - the (optional) initial value of the accumulator","title":"Parameters"},{"location":"api/foldl/#complexity-and-cost","text":"Cost : (1) costof(f) * p + sizeof(T) * p * g + l (2) costof(f) * (p + X) + sizeof(T) * p * g + l","title":"Complexity and cost"},{"location":"api/foldl/#bulkfoldl_each","text":"template < typename T , typename Func , typename S = T > std :: vector < T > foldl_each ( coarray < T >& xs , Func f , S start_value = {}) Perform a left-fold over each element of a coarray.","title":"bulk::foldl_each"},{"location":"api/foldl/#template-parameters_1","text":"T - the value type of the variable/array S - the value type of the accumulator Func - type of a binary function of the form (S&, T) -> void that modifies its first parameter, and is used in the fold","title":"Template parameters"},{"location":"api/foldl/#parameters_1","text":"xs - the coarray. Required the same local size on each processors, or an empty array is returned. f - the folding function start_value - the (optional) initial value of the accumulator","title":"Parameters"},{"location":"api/foldl/#result","text":"The i i th element of the result holds a left fold of f over the i i th element of each local image of the coarray xs .","title":"Result"},{"location":"api/foldl/#complexity-and-cost_1","text":"Cost : costof(f) * p + sizeof(T) * xs.size() * p * g + l","title":"Complexity and cost"},{"location":"api/foldl/#bulkmax","text":"template < typename T > T max ( bulk :: world & world , T t ); // (1) template < typename T > T max ( bulk :: var < T >& x ); // (2) template < typename T > T max ( bulk :: coarray < T >& xs ); // (3) Compute the global maximum of local values.","title":"bulk::max"},{"location":"api/foldl/#template-parameters_2","text":"T - the value type of the value/variable/array","title":"Template parameters"},{"location":"api/foldl/#parameters_2","text":"t - the value x - the distributed variable xs - the coarray","title":"Parameters"},{"location":"api/foldl/#complexity-and-cost_2","text":"Cost : (1), (2) p + sizeof(T) * p * g + l (3) xs.size() * (xs.size() * p + sizeof(T) * p * g) + l","title":"Complexity and cost"},{"location":"api/foldl/#bulkmin","text":"template < typename T > T min ( bulk :: world & world , T t ); // (1) template < typename T > T min ( bulk :: var < T >& x ); // (2) template < typename T > T min ( bulk :: coarray < T >& xs ); // (3) Compute the global minimum of local values.","title":"bulk::min"},{"location":"api/foldl/#template-parameters_3","text":"T - the value type of the value/variable/array","title":"Template parameters"},{"location":"api/foldl/#parameters_3","text":"t - the value x - the distributed variable xs - the coarray","title":"Parameters"},{"location":"api/foldl/#complexity-and-cost_3","text":"Cost : (1), (2) p + sizeof(T) * p * g + l (3) xs.size() * (xs.size() * p + sizeof(T) * p * g) + l","title":"Complexity and cost"},{"location":"api/foldl/#bulksum","text":"template < typename T > T sum ( bulk :: world & world , T t ); // (1) template < typename T > T sum ( bulk :: var < T >& x ); // (2) template < typename T > T sum ( bulk :: coarray < T >& xs ); // (3) Compute the global sum of local values.","title":"bulk::sum"},{"location":"api/foldl/#template-parameters_4","text":"T - the value type of the value/variable/array","title":"Template parameters"},{"location":"api/foldl/#parameters_4","text":"t - the value x - the distributed variable xs - the coarray","title":"Parameters"},{"location":"api/foldl/#complexity-and-cost_4","text":"Cost : (1), (2) p + sizeof(T) * p * g + l (3) xs.size() * (xs.size() * p + sizeof(T) * p * g) + l","title":"Complexity and cost"},{"location":"api/foldl/#bulkproduct","text":"template < typename T > T product ( bulk :: world & world , T t ); // (1) template < typename T > T product ( bulk :: var < T >& x ); // (2) template < typename T > T product ( bulk :: coarray < T >& xs ); // (3) Compute the global product of local values.","title":"bulk::product"},{"location":"api/foldl/#template-parameters_5","text":"T - the value type of the value/variable/array","title":"Template parameters"},{"location":"api/foldl/#parameters_5","text":"t - the value x - the distributed variable xs - the coarray","title":"Parameters"},{"location":"api/foldl/#complexity-and-cost_5","text":"Cost : (1), (2) p + sizeof(T) * p * g + l (3) xs.size() * (xs.size() * p + sizeof(T) * p * g) + l","title":"Complexity and cost"},{"location":"api/future/","text":"bulk::future Defined in header <bulk/future.hpp> . template < typename T > class future ; // (1) template < typename T > class future < T [] > ; // (2) bulk::future represents a value (1) or array (2) that will or has become known in the superstep after its creation. Template parameters T - the type of the value(s) stored in the future Member functions (constructor) constructs the future (deconstructor) deconstructs the future operator= assign a future Value access value returns the value of the future (1) operator[] return an element of the array (2) Hub access world returns the hub to which the future belongs","title":"future"},{"location":"api/future/#bulkfuture","text":"Defined in header <bulk/future.hpp> . template < typename T > class future ; // (1) template < typename T > class future < T [] > ; // (2) bulk::future represents a value (1) or array (2) that will or has become known in the superstep after its creation.","title":"bulk::future"},{"location":"api/future/#template-parameters","text":"T - the type of the value(s) stored in the future","title":"Template parameters"},{"location":"api/future/#member-functions","text":"(constructor) constructs the future (deconstructor) deconstructs the future operator= assign a future Value access value returns the value of the future (1) operator[] return an element of the array (2) Hub access world returns the hub to which the future belongs","title":"Member functions"},{"location":"api/gather_all/","text":"bulk::gather_all template < typename T > bulk :: coarray < T > gather_all ( bulk :: world & world , T value ) This function takes a value on each processor, and gathers all of these in the local images of a coarray. Template parameters T - the value type Parameters world - the world in which we gather value - the distributed variable Returns A coarray, whose local images consists of p elements with on the s -th position the value passed by the s -th processor. Complexity and cost Cost : sizeof(T) * p * g + l","title":"gather_all"},{"location":"api/gather_all/#bulkgather_all","text":"template < typename T > bulk :: coarray < T > gather_all ( bulk :: world & world , T value ) This function takes a value on each processor, and gathers all of these in the local images of a coarray.","title":"bulk::gather_all"},{"location":"api/gather_all/#template-parameters","text":"T - the value type","title":"Template parameters"},{"location":"api/gather_all/#parameters","text":"world - the world in which we gather value - the distributed variable","title":"Parameters"},{"location":"api/gather_all/#returns","text":"A coarray, whose local images consists of p elements with on the s -th position the value passed by the s -th processor.","title":"Returns"},{"location":"api/gather_all/#complexity-and-cost","text":"Cost : sizeof(T) * p * g + l","title":"Complexity and cost"},{"location":"api/partitioning/","text":"Partitionings bulk::index Defined in header <bulk/util/indices.hpp> . Array-like type for D > 1 , and wrapper around size_t for D == 1 . template < int D > struct index ; For backward compatibility, we have an alias index_type (deprecated). template < int D > using index_type = index < D > ; Note In Bulk, int is the type used for processor counts ands indices, and size_t is used for sizes and indices in containers. Partitionings therefore return int when computing ranks and owners, and size_t in all other cases. The only exception is multi_index which is represented as bulk::index and thus unsigned. bulk::partitioning Defined in header <bulk/partitionings/partitioning.hpp> . template < int D > class partitioning ; Base class for partitionings over a 1D processor grid. Template parameters D - the dimension of the data space Member functions (constructor) constructs the partitioning (deconstructor) deconstructs the partitioning global_size returns the global data shape local_size returns the local data shape local_count returns the number of local elements owner returns the owner of a global index local convert a global index to a local one global convert a local index to a global one bulk::multi_partitioning Defined in header <bulk/partitionings/partitioning.hpp> . template < int D , int G > class multi_partitioning : public partitioning < D > ; Base class for partitionings over a processor grid of dimension G . Template parameters D - the dimension of the data space G - the dimension of the processor grid Member functions (constructor) constructs the partitioning (deconstructor) deconstructs the partitioning local_size returns the local data shape global convert a local index to a global one multi_rank convert a 1D processor rank to its grid rank grid returns the processor grid shape rank convert a grid rank to a 1D processor rank multi_owner get the multi rank of a global index bulk::rectangular_partitioning Defined in header <bulk/partitionings/partitioning.hpp> . template < int D , int G > class rectangular_partitioning : public multi_partitioning < D , G > ; Base class for partitionings over a processor grid of dimension G , where the local data of each processor is a (hyper)rectangular subregion of the global data space. Template parameters D - the dimension of the data space G - the dimension of the processor grid Member functions (constructor) constructs the partitioning (deconstructor) deconstructs the partitioning origin returns the origin of the local subregion bulk::cartesian_partitioning Defined in header <bulk/partitionings/partitioning.hpp> . template < int D , int G = D > class cartesian_partitioning : public multi_partitioning < D , G > ; Base class for partitionings over a processor grid of dimension G , where the each axis is partitioned independently. Template parameters D - the dimension of the data space G - the dimension of the processor grid Member functions (constructor) constructs the partitioning (deconstructor) deconstructs the partitioning owner returns the owner of a global index local convert a global index to a local one global convert a local index to a global one local_size returns the local data shape bulk::cyclic_partitioning Defined in header <bulk/partitionings/cyclic.hpp> . template < int D , int G = D > class cyclic_partitioning : public cartesian_partitioning < D , G > ; A cyclic partitioning distributes the indices of a D-dimension space over the first G axes, where G is the dimensionality of the processor grid. bulk::block_partitioning Defined in header <bulk/partitionings/block.hpp> . template < int D , int G = D > class block_partitioning : public rectangular_partitioning < D , G > ; A block distribution. This equally block-distributes the first G axes. Optionally, a third argument axes , an array of size G that indicates the axes over which to partition, can be supplied to the constructor. bulk::tree_partitioning Defined in header <bulk/partitionings/tree.hpp> . template < int D > class tree_partitioning : public rectangular_partitioning < D , 1 > A binary-space partitioning (abbreviated BSP). (a_0, d_0) / \\ (a_10, d_10) (a_11, d_11) / \\ / \\ ... ... ... ... | | | | (a_n0, d_n0) ... ... (a_n(p/2), d_n(p/2)) represented as binary tree of splits. (constructor) tree_partitioning ( index_type < D > data_size , int procs , util :: binary_tree < util :: split >&& splits ) Parameters data_size - the shape of the data procs - the number of processors to distribute over splits - a binary tree, each node has associated a pair (a, d) with the location a , and axis d , among which is recursively split.","title":"partitioning"},{"location":"api/partitioning/#partitionings","text":"","title":"Partitionings"},{"location":"api/partitioning/#bulkindex","text":"Defined in header <bulk/util/indices.hpp> . Array-like type for D > 1 , and wrapper around size_t for D == 1 . template < int D > struct index ; For backward compatibility, we have an alias index_type (deprecated). template < int D > using index_type = index < D > ; Note In Bulk, int is the type used for processor counts ands indices, and size_t is used for sizes and indices in containers. Partitionings therefore return int when computing ranks and owners, and size_t in all other cases. The only exception is multi_index which is represented as bulk::index and thus unsigned.","title":"bulk::index"},{"location":"api/partitioning/#bulkpartitioning","text":"Defined in header <bulk/partitionings/partitioning.hpp> . template < int D > class partitioning ; Base class for partitionings over a 1D processor grid.","title":"bulk::partitioning"},{"location":"api/partitioning/#template-parameters","text":"D - the dimension of the data space","title":"Template parameters"},{"location":"api/partitioning/#member-functions","text":"(constructor) constructs the partitioning (deconstructor) deconstructs the partitioning global_size returns the global data shape local_size returns the local data shape local_count returns the number of local elements owner returns the owner of a global index local convert a global index to a local one global convert a local index to a global one","title":"Member functions"},{"location":"api/partitioning/#bulkmulti_partitioning","text":"Defined in header <bulk/partitionings/partitioning.hpp> . template < int D , int G > class multi_partitioning : public partitioning < D > ; Base class for partitionings over a processor grid of dimension G .","title":"bulk::multi_partitioning"},{"location":"api/partitioning/#template-parameters_1","text":"D - the dimension of the data space G - the dimension of the processor grid","title":"Template parameters"},{"location":"api/partitioning/#member-functions_1","text":"(constructor) constructs the partitioning (deconstructor) deconstructs the partitioning local_size returns the local data shape global convert a local index to a global one multi_rank convert a 1D processor rank to its grid rank grid returns the processor grid shape rank convert a grid rank to a 1D processor rank multi_owner get the multi rank of a global index","title":"Member functions"},{"location":"api/partitioning/#bulkrectangular_partitioning","text":"Defined in header <bulk/partitionings/partitioning.hpp> . template < int D , int G > class rectangular_partitioning : public multi_partitioning < D , G > ; Base class for partitionings over a processor grid of dimension G , where the local data of each processor is a (hyper)rectangular subregion of the global data space.","title":"bulk::rectangular_partitioning"},{"location":"api/partitioning/#template-parameters_2","text":"D - the dimension of the data space G - the dimension of the processor grid","title":"Template parameters"},{"location":"api/partitioning/#member-functions_2","text":"(constructor) constructs the partitioning (deconstructor) deconstructs the partitioning origin returns the origin of the local subregion","title":"Member functions"},{"location":"api/partitioning/#bulkcartesian_partitioning","text":"Defined in header <bulk/partitionings/partitioning.hpp> . template < int D , int G = D > class cartesian_partitioning : public multi_partitioning < D , G > ; Base class for partitionings over a processor grid of dimension G , where the each axis is partitioned independently.","title":"bulk::cartesian_partitioning"},{"location":"api/partitioning/#template-parameters_3","text":"D - the dimension of the data space G - the dimension of the processor grid","title":"Template parameters"},{"location":"api/partitioning/#member-functions_3","text":"(constructor) constructs the partitioning (deconstructor) deconstructs the partitioning owner returns the owner of a global index local convert a global index to a local one global convert a local index to a global one local_size returns the local data shape","title":"Member functions"},{"location":"api/partitioning/#bulkcyclic_partitioning","text":"Defined in header <bulk/partitionings/cyclic.hpp> . template < int D , int G = D > class cyclic_partitioning : public cartesian_partitioning < D , G > ; A cyclic partitioning distributes the indices of a D-dimension space over the first G axes, where G is the dimensionality of the processor grid.","title":"bulk::cyclic_partitioning"},{"location":"api/partitioning/#bulkblock_partitioning","text":"Defined in header <bulk/partitionings/block.hpp> . template < int D , int G = D > class block_partitioning : public rectangular_partitioning < D , G > ; A block distribution. This equally block-distributes the first G axes. Optionally, a third argument axes , an array of size G that indicates the axes over which to partition, can be supplied to the constructor.","title":"bulk::block_partitioning"},{"location":"api/partitioning/#bulktree_partitioning","text":"Defined in header <bulk/partitionings/tree.hpp> . template < int D > class tree_partitioning : public rectangular_partitioning < D , 1 > A binary-space partitioning (abbreviated BSP). (a_0, d_0) / \\ (a_10, d_10) (a_11, d_11) / \\ / \\ ... ... ... ... | | | | (a_n0, d_n0) ... ... (a_n(p/2), d_n(p/2)) represented as binary tree of splits.","title":"bulk::tree_partitioning"},{"location":"api/partitioning/#constructor","text":"tree_partitioning ( index_type < D > data_size , int procs , util :: binary_tree < util :: split >&& splits )","title":"(constructor)"},{"location":"api/partitioning/#parameters","text":"data_size - the shape of the data procs - the number of processors to distribute over splits - a binary tree, each node has associated a pair (a, d) with the location a , and axis d , among which is recursively split.","title":"Parameters"},{"location":"api/queue/","text":"bulk::queue Defined in header <bulk/messages.hpp> . template < typename T , typename ... Ts > class queue ; bulk::queue is an inbox for receiving messages passed by processors. Template parameters T - the type of the message content Ts - a parameter pack, optionally containing extra message content Note: content components of array type, e.g. T[] are also supported. They are represented by a std::vector<T> . Member types message_type : the underlying type used by the messages. Equal to: T if sizeof...(Ts) == 0 , std::tuple<T, Ts...> otherwise. iterator : the type of the iterator used for the local inbox Member functions (constructor) constructs the queue (deconstructor) deconstructs the queue operator= assign the queue Communication operator() obtain a sender to a remote queue Container begin obtain an iterator to the start end obtain an iterator to the end size obtain the number of messages empty check if the queue is empty World access world returns the world of the queue Nested classes sender : an object providing syntactic sugar for sending messages Example #include \"bulk/bulk.hpp\" #include \"set_backend.hpp\" int main () { environment env ; env . spawn ( env . available_processors (), []( bulk :: world & world ) { auto raw_queue = bulk :: queue < int > ( world ); raw_queue ( world . next_processor ()). send ( 1 ); raw_queue ( world . next_processor ()). send ( 2 ); raw_queue ( world . next_processor ()). send ( 123 ); auto tuple_queue = bulk :: queue < int , int , int > ( world ); tuple_queue ( world . next_processor ()). send ( 1 , 2 , 3 ); tuple_queue ( world . next_processor ()). send ( 4 , 5 , 6 ); world . sync (); // read queue for ( auto x : raw_queue ) { world . log ( \"the first queue received a message: %d\" , x ); } for ( auto [ i , j , k ] : tuple_queue ) { world . log ( \"the second queue received a tuple: (%d, %d, %d)\" , i , j , k ); } }); return 0 ; }","title":"queue"},{"location":"api/queue/#bulkqueue","text":"Defined in header <bulk/messages.hpp> . template < typename T , typename ... Ts > class queue ; bulk::queue is an inbox for receiving messages passed by processors.","title":"bulk::queue"},{"location":"api/queue/#template-parameters","text":"T - the type of the message content Ts - a parameter pack, optionally containing extra message content Note: content components of array type, e.g. T[] are also supported. They are represented by a std::vector<T> .","title":"Template parameters"},{"location":"api/queue/#member-types","text":"message_type : the underlying type used by the messages. Equal to: T if sizeof...(Ts) == 0 , std::tuple<T, Ts...> otherwise. iterator : the type of the iterator used for the local inbox","title":"Member types"},{"location":"api/queue/#member-functions","text":"(constructor) constructs the queue (deconstructor) deconstructs the queue operator= assign the queue Communication operator() obtain a sender to a remote queue Container begin obtain an iterator to the start end obtain an iterator to the end size obtain the number of messages empty check if the queue is empty World access world returns the world of the queue","title":"Member functions"},{"location":"api/queue/#nested-classes","text":"sender : an object providing syntactic sugar for sending messages","title":"Nested classes"},{"location":"api/queue/#example","text":"#include \"bulk/bulk.hpp\" #include \"set_backend.hpp\" int main () { environment env ; env . spawn ( env . available_processors (), []( bulk :: world & world ) { auto raw_queue = bulk :: queue < int > ( world ); raw_queue ( world . next_processor ()). send ( 1 ); raw_queue ( world . next_processor ()). send ( 2 ); raw_queue ( world . next_processor ()). send ( 123 ); auto tuple_queue = bulk :: queue < int , int , int > ( world ); tuple_queue ( world . next_processor ()). send ( 1 , 2 , 3 ); tuple_queue ( world . next_processor ()). send ( 4 , 5 , 6 ); world . sync (); // read queue for ( auto x : raw_queue ) { world . log ( \"the first queue received a message: %d\" , x ); } for ( auto [ i , j , k ] : tuple_queue ) { world . log ( \"the second queue received a tuple: (%d, %d, %d)\" , i , j , k ); } }); return 0 ; }","title":"Example"},{"location":"api/timer/","text":"bulk::util::timer Defined in header <bulk/util/timer.hpp> . class timer ; Constructs a timer that immediately starts ticking. bulk::util::timer::get template < typename resolution = std :: milli > double get () returns the number of (by default) milliseconds that have passed since the construction of the timer. bulk::util::timer::get_change template < typename resolution = std :: milli > double get_change () returns the number of (by default) milliseconds that have passed since the last call to get_change , or (for the first call) since the construction of the timer.","title":"timer"},{"location":"api/timer/#bulkutiltimer","text":"Defined in header <bulk/util/timer.hpp> . class timer ; Constructs a timer that immediately starts ticking.","title":"bulk::util::timer"},{"location":"api/timer/#bulkutiltimerget","text":"template < typename resolution = std :: milli > double get () returns the number of (by default) milliseconds that have passed since the construction of the timer.","title":"bulk::util::timer::get"},{"location":"api/timer/#bulkutiltimerget_change","text":"template < typename resolution = std :: milli > double get_change () returns the number of (by default) milliseconds that have passed since the last call to get_change , or (for the first call) since the construction of the timer.","title":"bulk::util::timer::get_change"},{"location":"api/unflatten/","text":"bulk::util::unflatten template < int D > index < D > unflatten ( index < D > volume , int flattened ); Unflatten a multi-index in a D-dimensional volume. Template parameters D - the dimension of the index space Parameters volume - the size of the volume idxs - the multi-index","title":"unflatten"},{"location":"api/unflatten/#bulkutilunflatten","text":"template < int D > index < D > unflatten ( index < D > volume , int flattened ); Unflatten a multi-index in a D-dimensional volume.","title":"bulk::util::unflatten"},{"location":"api/unflatten/#template-parameters","text":"D - the dimension of the index space","title":"Template parameters"},{"location":"api/unflatten/#parameters","text":"volume - the size of the volume idxs - the multi-index","title":"Parameters"},{"location":"api/var/","text":"bulk::var Defined in header <bulk/variable.hpp> . template < typename T > class var ; bulk::var represents a distributed object with an image on each processor. This image is readable and writable from remote processors. Template parameters T - the type of the values stored in the images of the variable. Member types value_type : the type of the distributed data (i.e. T ) Member functions (constructor) constructs the variable (deconstructor) deconstructs the variable operator= assign values to the variable Value access value returns the value of the local variable image operator T implicit cast to value reference Communication operator() obtain an image to a remote value broadcast broadcast a value to all remote images World access world returns the world of the variable Nested classes image : an object providing syntactic sugar for reading and writing to images. Example #include \"bulk/bulk.hpp\" #include \"set_backend.hpp\" int main () { environment env ; env . spawn ( env . available_processors (), []( bulk :: world & world ) { int s = world . processor_id (); int p = world . active_processors (); bulk :: var < int > a ( world ); a ( world . next_processor ()) = s ; world . sync (); world . log ( \"%d/%d <- %d\" , s , p , a . value ()); auto b = a ( world . next_processor ()). get (); world . sync (); world . log ( \"%d/%d -> %d\" , s , p , b . value ()); }); return 0 ; }","title":"var"},{"location":"api/var/#bulkvar","text":"Defined in header <bulk/variable.hpp> . template < typename T > class var ; bulk::var represents a distributed object with an image on each processor. This image is readable and writable from remote processors.","title":"bulk::var"},{"location":"api/var/#template-parameters","text":"T - the type of the values stored in the images of the variable.","title":"Template parameters"},{"location":"api/var/#member-types","text":"value_type : the type of the distributed data (i.e. T )","title":"Member types"},{"location":"api/var/#member-functions","text":"(constructor) constructs the variable (deconstructor) deconstructs the variable operator= assign values to the variable Value access value returns the value of the local variable image operator T implicit cast to value reference Communication operator() obtain an image to a remote value broadcast broadcast a value to all remote images World access world returns the world of the variable","title":"Member functions"},{"location":"api/var/#nested-classes","text":"image : an object providing syntactic sugar for reading and writing to images.","title":"Nested classes"},{"location":"api/var/#example","text":"#include \"bulk/bulk.hpp\" #include \"set_backend.hpp\" int main () { environment env ; env . spawn ( env . available_processors (), []( bulk :: world & world ) { int s = world . processor_id (); int p = world . active_processors (); bulk :: var < int > a ( world ); a ( world . next_processor ()) = s ; world . sync (); world . log ( \"%d/%d <- %d\" , s , p , a . value ()); auto b = a ( world . next_processor ()). get (); world . sync (); world . log ( \"%d/%d -> %d\" , s , p , b . value ()); }); return 0 ; }","title":"Example"},{"location":"api/world/","text":"bulk::world Defined in header <bulk/world.hpp> . class world ; bulk::world represents the world of a processor and its place within it, by providing information and mechanisms to communicate with other processors, or obtain information about the local processor. Member functions System information active_processors returns the number of active processors rank returns the id of the local processor next_rank returns the id of the next logical processor prev_rank returns the id of the previous logical processor Communication and coordination sync performs a bulk synchronization barrier performs a bulk barrier split split the world in subsets Logging log log a message log_once log a message only with the first processor Example #include \"bulk/bulk.hpp\" #include \"set_backend.hpp\" int main () { environment env ; env . spawn ( env . available_processors (), []( bulk :: world & world ) { int s = world . rank (); int p = world . active_processors (); world . log ( \"Hello, world! %d/%d\" , s , p ); }); return 0 ; }","title":"world"},{"location":"api/world/#bulkworld","text":"Defined in header <bulk/world.hpp> . class world ; bulk::world represents the world of a processor and its place within it, by providing information and mechanisms to communicate with other processors, or obtain information about the local processor.","title":"bulk::world"},{"location":"api/world/#member-functions","text":"System information active_processors returns the number of active processors rank returns the id of the local processor next_rank returns the id of the next logical processor prev_rank returns the id of the previous logical processor Communication and coordination sync performs a bulk synchronization barrier performs a bulk barrier split split the world in subsets Logging log log a message log_once log a message only with the first processor","title":"Member functions"},{"location":"api/world/#example","text":"#include \"bulk/bulk.hpp\" #include \"set_backend.hpp\" int main () { environment env ; env . spawn ( env . available_processors (), []( bulk :: world & world ) { int s = world . rank (); int p = world . active_processors (); world . log ( \"Hello, world! %d/%d\" , s , p ); }); return 0 ; }","title":"Example"},{"location":"api/cartesian_partitioning/constructor/","text":"bulk::cartesian_partitioning::cartesian_partitioning cartesian_partitioning ( index_type < D > global_size , index_type < G > grid_size ); Constructs a Cartesian partitioning. Parameters global_size - the shape of the data grid_size - the shape of the processor grid","title":"cartesian_partitioning::constructor"},{"location":"api/cartesian_partitioning/constructor/#bulkcartesian_partitioningcartesian_partitioning","text":"cartesian_partitioning ( index_type < D > global_size , index_type < G > grid_size ); Constructs a Cartesian partitioning.","title":"bulk::cartesian_partitioning::cartesian_partitioning"},{"location":"api/cartesian_partitioning/constructor/#parameters","text":"global_size - the shape of the data grid_size - the shape of the processor grid","title":"Parameters"},{"location":"api/cartesian_partitioning/deconstructor/","text":"bulk::cartesian_partitioning::~cartesian_partitioning virtual ~ cartesian_partitioning () = default ; Virtual deconstructor.","title":"cartesian_partitioning::deconstructor"},{"location":"api/cartesian_partitioning/deconstructor/#bulkcartesian_partitioningcartesian_partitioning","text":"virtual ~ cartesian_partitioning () = default ; Virtual deconstructor.","title":"bulk::cartesian_partitioning::~cartesian_partitioning"},{"location":"api/cartesian_partitioning/global/","text":"bulk::cartesian_partitioning::global using multi_partitioning < D , G >:: global ; virtual int global ( int g , int u , int i ); // overload Convert a local index to a global one (overload). Parameters g : the axis for which to compute the index u : the g th component of the local multi rank i : the g th component of the local index Return value int : the g th component of the global index See also partitioning::global","title":"cartesian_partitioning::global"},{"location":"api/cartesian_partitioning/global/#bulkcartesian_partitioningglobal","text":"using multi_partitioning < D , G >:: global ; virtual int global ( int g , int u , int i ); // overload Convert a local index to a global one (overload).","title":"bulk::cartesian_partitioning::global"},{"location":"api/cartesian_partitioning/global/#parameters","text":"g : the axis for which to compute the index u : the g th component of the local multi rank i : the g th component of the local index","title":"Parameters"},{"location":"api/cartesian_partitioning/global/#return-value","text":"int : the g th component of the global index","title":"Return value"},{"location":"api/cartesian_partitioning/global/#see-also","text":"partitioning::global","title":"See also"},{"location":"api/cartesian_partitioning/local/","text":"bulk::cartesian_partitioning::local using multi_partitioning < D , G >:: local ; virtual int local ( int g , int i ); // overload Convert a global index to a local one (overload). Parameters g : the axis for which to compute the index i : the g th component of the global index Return value int : the g th component of the local index See also partitioning::local","title":"cartesian_partitioning::local"},{"location":"api/cartesian_partitioning/local/#bulkcartesian_partitioninglocal","text":"using multi_partitioning < D , G >:: local ; virtual int local ( int g , int i ); // overload Convert a global index to a local one (overload).","title":"bulk::cartesian_partitioning::local"},{"location":"api/cartesian_partitioning/local/#parameters","text":"g : the axis for which to compute the index i : the g th component of the global index","title":"Parameters"},{"location":"api/cartesian_partitioning/local/#return-value","text":"int : the g th component of the local index","title":"Return value"},{"location":"api/cartesian_partitioning/local/#see-also","text":"partitioning::local","title":"See also"},{"location":"api/cartesian_partitioning/local_size/","text":"bulk::multi_partitioning::local_size using multi_partitioning < D , G >:: local_size ; virtual int local_size ( int g , int u ); // overload Returns the local data space shape for a given processor (overload). Parameters g : the axis for which to compute the index u : the g th component of the local multi rank Return value int : The g th component of the shape of the local data space. See also partitioning::local_size","title":"cartesian_partitioning::local_size"},{"location":"api/cartesian_partitioning/local_size/#bulkmulti_partitioninglocal_size","text":"using multi_partitioning < D , G >:: local_size ; virtual int local_size ( int g , int u ); // overload Returns the local data space shape for a given processor (overload).","title":"bulk::multi_partitioning::local_size"},{"location":"api/cartesian_partitioning/local_size/#parameters","text":"g : the axis for which to compute the index u : the g th component of the local multi rank","title":"Parameters"},{"location":"api/cartesian_partitioning/local_size/#return-value","text":"int : The g th component of the shape of the local data space.","title":"Return value"},{"location":"api/cartesian_partitioning/local_size/#see-also","text":"partitioning::local_size","title":"See also"},{"location":"api/cartesian_partitioning/owner/","text":"bulk::cartesian_partitioning::owner using multi_partitioning < D , G >:: owner ; virtual int owner ( int g , int i ); Returns the rank of the processor that owns the element at a given index. Parameters g : the axis for which to compute the index i : the g th component of the local index Return value int : the g th component of the multi rank of the owner See also partitioning::owner","title":"cartesian_partitioning::owner"},{"location":"api/cartesian_partitioning/owner/#bulkcartesian_partitioningowner","text":"using multi_partitioning < D , G >:: owner ; virtual int owner ( int g , int i ); Returns the rank of the processor that owns the element at a given index.","title":"bulk::cartesian_partitioning::owner"},{"location":"api/cartesian_partitioning/owner/#parameters","text":"g : the axis for which to compute the index i : the g th component of the local index","title":"Parameters"},{"location":"api/cartesian_partitioning/owner/#return-value","text":"int : the g th component of the multi rank of the owner","title":"Return value"},{"location":"api/cartesian_partitioning/owner/#see-also","text":"partitioning::owner","title":"See also"},{"location":"api/coarray/constructor/","text":"bulk::coarray::coarray coarray ( world & world , int local_size ); // (1) coarray ( world & world , int local_size , T default_value ); // (2) Constructs a coarray of size p x local_size and registers it with world . \u2026 In addition, also initializes the elements as default_value . Parameters world - the world this variable belongs to local_size - the size of the coarray image of the local processor default_value - the initial value of the local coarray elements Complexity and cost Cost - l or free (backend dependent)","title":"coarray::constructor"},{"location":"api/coarray/constructor/#bulkcoarraycoarray","text":"coarray ( world & world , int local_size ); // (1) coarray ( world & world , int local_size , T default_value ); // (2) Constructs a coarray of size p x local_size and registers it with world . \u2026 In addition, also initializes the elements as default_value .","title":"bulk::coarray::coarray"},{"location":"api/coarray/constructor/#parameters","text":"world - the world this variable belongs to local_size - the size of the coarray image of the local processor default_value - the initial value of the local coarray elements","title":"Parameters"},{"location":"api/coarray/constructor/#complexity-and-cost","text":"Cost - l or free (backend dependent)","title":"Complexity and cost"},{"location":"api/coarray/deconstructor/","text":"bulk::coarray::~coarray ~ coarray (); Deconstructs a coarray and deregisters it with its world . Complexity and cost Cost - l or free (backend dependent)","title":"coarray::deconstructor"},{"location":"api/coarray/deconstructor/#bulkcoarraycoarray","text":"~ coarray (); Deconstructs a coarray and deregisters it with its world .","title":"bulk::coarray::~coarray"},{"location":"api/coarray/deconstructor/#complexity-and-cost","text":"Cost - l or free (backend dependent)","title":"Complexity and cost"},{"location":"api/coarray/image/","text":"bulk::coarray::image Defined in header <bulk/coarray.hpp> . class image ; A coarray image, allows for remote element access Member functions operator[] returns a writer to a remote image element bulk::coarray::image::operator[] writer operator []( int i ); parameters i - the index of the image element returns a bulk::coarray::writer to the remote image element","title":"coarray::image"},{"location":"api/coarray/image/#bulkcoarrayimage","text":"Defined in header <bulk/coarray.hpp> . class image ; A coarray image, allows for remote element access","title":"bulk::coarray::image"},{"location":"api/coarray/image/#member-functions","text":"operator[] returns a writer to a remote image element","title":"Member functions"},{"location":"api/coarray/image/#bulkcoarrayimageoperator","text":"writer operator []( int i ); parameters i - the index of the image element returns a bulk::coarray::writer to the remote image element","title":"bulk::coarray::image::operator[]"},{"location":"api/coarray/parentheses_operator/","text":"bulk::coarray::operator() image operator ()( int t ); Obtain an object encapsulating the image of the coarray. Parameters t - the image index","title":"coarray::operator()"},{"location":"api/coarray/parentheses_operator/#bulkcoarrayoperator","text":"image operator ()( int t ); Obtain an object encapsulating the image of the coarray.","title":"bulk::coarray::operator()"},{"location":"api/coarray/parentheses_operator/#parameters","text":"t - the image index","title":"Parameters"},{"location":"api/coarray/slice/","text":"bulk::coarray::slice Defined in header <bulk/coarray.hpp> . struct slice { int first ; int last ; };","title":"coarray::slice"},{"location":"api/coarray/slice/#bulkcoarrayslice","text":"Defined in header <bulk/coarray.hpp> . struct slice { int first ; int last ; };","title":"bulk::coarray::slice"},{"location":"api/coarray/slice_writer/","text":"bulk::coarray::slice_writer Defined in header <bulk/coarray.hpp> . class slice_writer ; Modify remote slices. Member functions operator= assign a value to a remote element get get a future to a slice bulk::coarray::slice_writer::operator= void operator = ( const std :: vector < T >& values ); parameters values - the new values of the slice bulk::coarray::slice_writer::get future < T [] > get (); returns a future array containing the slice","title":"coarray::slice_writer"},{"location":"api/coarray/slice_writer/#bulkcoarrayslice_writer","text":"Defined in header <bulk/coarray.hpp> . class slice_writer ; Modify remote slices.","title":"bulk::coarray::slice_writer"},{"location":"api/coarray/slice_writer/#member-functions","text":"operator= assign a value to a remote element get get a future to a slice","title":"Member functions"},{"location":"api/coarray/slice_writer/#bulkcoarrayslice_writeroperator","text":"void operator = ( const std :: vector < T >& values ); parameters values - the new values of the slice","title":"bulk::coarray::slice_writer::operator="},{"location":"api/coarray/slice_writer/#bulkcoarrayslice_writerget","text":"future < T [] > get (); returns a future array containing the slice","title":"bulk::coarray::slice_writer::get"},{"location":"api/coarray/square_brackets_operator/","text":"bulk::coarray::operator[] T & operator []( int i ); Access the i-th element of the local coarray image Returns A reference to the i-th element of the local image. Parameters i - the element index","title":"coarray::operator[]"},{"location":"api/coarray/square_brackets_operator/#bulkcoarrayoperator","text":"T & operator []( int i ); Access the i-th element of the local coarray image","title":"bulk::coarray::operator[]"},{"location":"api/coarray/square_brackets_operator/#returns","text":"A reference to the i-th element of the local image.","title":"Returns"},{"location":"api/coarray/square_brackets_operator/#parameters","text":"i - the element index","title":"Parameters"},{"location":"api/coarray/world/","text":"bulk::coarray::world bulk :: world & world (); Returns a reference to the world the variable belongs to Return value A reference to the world.","title":"coarray::world"},{"location":"api/coarray/world/#bulkcoarrayworld","text":"bulk :: world & world (); Returns a reference to the world the variable belongs to","title":"bulk::coarray::world"},{"location":"api/coarray/world/#return-value","text":"A reference to the world.","title":"Return value"},{"location":"api/coarray/writer/","text":"bulk::coarray::writer Defined in header <bulk/coarray.hpp> . class writer ; A coarray image writer, allows for the modification of remote elements. Member functions operator= assign a value to a remote element get obtain a remote value bulk::coarray::writer::operator= void operator = ( T value ); parameters value - the new value of the element bulk::coarray::writer::get future < T > get (); returns a future to the element","title":"coarray::writer"},{"location":"api/coarray/writer/#bulkcoarraywriter","text":"Defined in header <bulk/coarray.hpp> . class writer ; A coarray image writer, allows for the modification of remote elements.","title":"bulk::coarray::writer"},{"location":"api/coarray/writer/#member-functions","text":"operator= assign a value to a remote element get obtain a remote value","title":"Member functions"},{"location":"api/coarray/writer/#bulkcoarraywriteroperator","text":"void operator = ( T value ); parameters value - the new value of the element","title":"bulk::coarray::writer::operator="},{"location":"api/coarray/writer/#bulkcoarraywriterget","text":"future < T > get (); returns a future to the element","title":"bulk::coarray::writer::get"},{"location":"api/environment/available_processors/","text":"bulk::environment::available_processors int available_processors () const ; Returns the total number of processors available on the system. Return value int : The number of available processors.","title":"environment::available_processors"},{"location":"api/environment/available_processors/#bulkenvironmentavailable_processors","text":"int available_processors () const ; Returns the total number of processors available on the system.","title":"bulk::environment::available_processors"},{"location":"api/environment/available_processors/#return-value","text":"int : The number of available processors.","title":"Return value"},{"location":"api/environment/spawn/","text":"bulk::environment::spawn void spawn ( int processors , std :: function < void ( bulk :: world & ) > spmd ); Start an SPMD section on a given number of processors. Parameters processors - the number of processors to run the SPMD section on. spmd - the SPMD function that gets run on each (virtual) processor. A reference to the world of the processor will be passed as the first and only argument.","title":"environment::spawn"},{"location":"api/environment/spawn/#bulkenvironmentspawn","text":"void spawn ( int processors , std :: function < void ( bulk :: world & ) > spmd ); Start an SPMD section on a given number of processors.","title":"bulk::environment::spawn"},{"location":"api/environment/spawn/#parameters","text":"processors - the number of processors to run the SPMD section on. spmd - the SPMD function that gets run on each (virtual) processor. A reference to the world of the processor will be passed as the first and only argument.","title":"Parameters"},{"location":"api/future/assignment_operator/","text":"bulk::future::operator= void operator = ( future < T >&& other ); Move assignment operator. Replaces the target future *this with the source future other , and invalidates the source. Parameters other - another future to move away from","title":"future::operator="},{"location":"api/future/assignment_operator/#bulkfutureoperator","text":"void operator = ( future < T >&& other ); Move assignment operator. Replaces the target future *this with the source future other , and invalidates the source.","title":"bulk::future::operator="},{"location":"api/future/assignment_operator/#parameters","text":"other - another future to move away from","title":"Parameters"},{"location":"api/future/bracket_operator/","text":"bulk::future<T[]>::operator[] T & operator []( int i ); Access the i-th element of the array image of the future. Returns A reference to the i-th element of the future. Parameters i - the element index","title":"future::operator[]"},{"location":"api/future/bracket_operator/#bulkfuturetoperator","text":"T & operator []( int i ); Access the i-th element of the array image of the future.","title":"bulk::future&lt;T[]&gt;::operator[]"},{"location":"api/future/bracket_operator/#returns","text":"A reference to the i-th element of the future.","title":"Returns"},{"location":"api/future/bracket_operator/#parameters","text":"i - the element index","title":"Parameters"},{"location":"api/future/constructor/","text":"bulk::future::future future ( world & world ); Constructs a message future for use in world . Parameters world - the world this future belongs to","title":"future::constructor"},{"location":"api/future/constructor/#bulkfuturefuture","text":"future ( world & world ); Constructs a message future for use in world .","title":"bulk::future::future"},{"location":"api/future/constructor/#parameters","text":"world - the world this future belongs to","title":"Parameters"},{"location":"api/future/deconstructor/","text":"bulk::future::~future ~ future (); Deconstructs a future and invalidates its buffer.","title":"future::deconstructor"},{"location":"api/future/deconstructor/#bulkfuturefuture","text":"~ future (); Deconstructs a future and invalidates its buffer.","title":"bulk::future::~future"},{"location":"api/future/value/","text":"bulk::future::value T & value (); Returns a reference to the value held by the future Return value A reference to the value Note This becomes valid after the next global synchronisation upon the initialization of the value of the future using e.g. bulk::get .","title":"future::value"},{"location":"api/future/value/#bulkfuturevalue","text":"T & value (); Returns a reference to the value held by the future","title":"bulk::future::value"},{"location":"api/future/value/#return-value","text":"A reference to the value","title":"Return value"},{"location":"api/future/value/#note","text":"This becomes valid after the next global synchronisation upon the initialization of the value of the future using e.g. bulk::get .","title":"Note"},{"location":"api/future/world/","text":"bulk::future::world bulk :: world & world (); Returns a reference to the world the future belongs to Return value A reference to the world.","title":"future::world"},{"location":"api/future/world/#bulkfutureworld","text":"bulk :: world & world (); Returns a reference to the world the future belongs to","title":"bulk::future::world"},{"location":"api/future/world/#return-value","text":"A reference to the world.","title":"Return value"},{"location":"api/multi_partitioning/constructor/","text":"bulk::multi_partitioning::multi_partitioning multi_partitioning ( index_type < D > global_size , index_type < G > grid_size ); Constructs a multi partitioning. Parameters global_size - the shape of the data grid_size - the shape of the processor grid","title":"multi_partitioning::constructor"},{"location":"api/multi_partitioning/constructor/#bulkmulti_partitioningmulti_partitioning","text":"multi_partitioning ( index_type < D > global_size , index_type < G > grid_size ); Constructs a multi partitioning.","title":"bulk::multi_partitioning::multi_partitioning"},{"location":"api/multi_partitioning/constructor/#parameters","text":"global_size - the shape of the data grid_size - the shape of the processor grid","title":"Parameters"},{"location":"api/multi_partitioning/deconstructor/","text":"bulk::multi_partitioning::~multi_partitioning virtual ~ multi_partitioning () = default ; Virtual deconstructor.","title":"multi_partitioning::deconstructor"},{"location":"api/multi_partitioning/deconstructor/#bulkmulti_partitioningmulti_partitioning","text":"virtual ~ multi_partitioning () = default ; Virtual deconstructor.","title":"bulk::multi_partitioning::~multi_partitioning"},{"location":"api/multi_partitioning/global/","text":"bulk::multi_partitioning::global index_type < D > global ( index_type < D > xs , int processor ) override ; virtual index_type < D > global ( index_type < D > xs , index_type < G > processor ); // overload Convert a local index to a global one (overload). Parameters processor : the local multi rank xs : the local index Return value index_type<D> : the global index See also partitioning::global","title":"multi_partitioning::global"},{"location":"api/multi_partitioning/global/#bulkmulti_partitioningglobal","text":"index_type < D > global ( index_type < D > xs , int processor ) override ; virtual index_type < D > global ( index_type < D > xs , index_type < G > processor ); // overload Convert a local index to a global one (overload).","title":"bulk::multi_partitioning::global"},{"location":"api/multi_partitioning/global/#parameters","text":"processor : the local multi rank xs : the local index","title":"Parameters"},{"location":"api/multi_partitioning/global/#return-value","text":"index_type<D> : the global index","title":"Return value"},{"location":"api/multi_partitioning/global/#see-also","text":"partitioning::global","title":"See also"},{"location":"api/multi_partitioning/grid/","text":"bulk::multi_partitioning::grid index_type < G > grid (); Returns the processor grid space. Return value index_type<G> : The shape of the processor grid.","title":"multi_partitioning::grid"},{"location":"api/multi_partitioning/grid/#bulkmulti_partitioninggrid","text":"index_type < G > grid (); Returns the processor grid space.","title":"bulk::multi_partitioning::grid"},{"location":"api/multi_partitioning/grid/#return-value","text":"index_type<G> : The shape of the processor grid.","title":"Return value"},{"location":"api/multi_partitioning/local_size/","text":"bulk::multi_partitioning::local_size index_type < D > local_size ( int processor ) override ; virtual index_type < D > local_size ( index_type < G > processor ) // overload Returns the local data space shape for a given processor (overload). Parameters processor : the multi rank for which to return the local shape. Return value index_type<D> : The shape of the local data space. See also partitioning::local_size","title":"multi_partitioning::local_size"},{"location":"api/multi_partitioning/local_size/#bulkmulti_partitioninglocal_size","text":"index_type < D > local_size ( int processor ) override ; virtual index_type < D > local_size ( index_type < G > processor ) // overload Returns the local data space shape for a given processor (overload).","title":"bulk::multi_partitioning::local_size"},{"location":"api/multi_partitioning/local_size/#parameters","text":"processor : the multi rank for which to return the local shape.","title":"Parameters"},{"location":"api/multi_partitioning/local_size/#return-value","text":"index_type<D> : The shape of the local data space.","title":"Return value"},{"location":"api/multi_partitioning/local_size/#see-also","text":"partitioning::local_size","title":"See also"},{"location":"api/multi_partitioning/multi_owner/","text":"bulk::partitioning::multi_owner int owner ( index_type < D > xs ) override ; virtual index_type < G > multi_owner ( index_type < D > xs ) // overload Returns the multi rank of the processor that owns the element at a given index. Parameters xs : the index in the data space Return value index_type<G> : the multi rank of the processor that owns xs See also partitioning::owner","title":"multi_partitioning::multi_owner"},{"location":"api/multi_partitioning/multi_owner/#bulkpartitioningmulti_owner","text":"int owner ( index_type < D > xs ) override ; virtual index_type < G > multi_owner ( index_type < D > xs ) // overload Returns the multi rank of the processor that owns the element at a given index.","title":"bulk::partitioning::multi_owner"},{"location":"api/multi_partitioning/multi_owner/#parameters","text":"xs : the index in the data space","title":"Parameters"},{"location":"api/multi_partitioning/multi_owner/#return-value","text":"index_type<G> : the multi rank of the processor that owns xs","title":"Return value"},{"location":"api/multi_partitioning/multi_owner/#see-also","text":"partitioning::owner","title":"See also"},{"location":"api/multi_partitioning/multi_rank/","text":"bulk::multi_partitioning::multi_rank index_type < G > multi_rank ( int t ); Convert a standard rank to a multi rank. Parameters t : the rank to convert Return value index_type<G> : the corresponding multi rank See also multi_partitioning::rank","title":"multi_partitioning::multi_rank"},{"location":"api/multi_partitioning/multi_rank/#bulkmulti_partitioningmulti_rank","text":"index_type < G > multi_rank ( int t ); Convert a standard rank to a multi rank.","title":"bulk::multi_partitioning::multi_rank"},{"location":"api/multi_partitioning/multi_rank/#parameters","text":"t : the rank to convert","title":"Parameters"},{"location":"api/multi_partitioning/multi_rank/#return-value","text":"index_type<G> : the corresponding multi rank","title":"Return value"},{"location":"api/multi_partitioning/multi_rank/#see-also","text":"multi_partitioning::rank","title":"See also"},{"location":"api/multi_partitioning/rank/","text":"bulk::multi_partitioning::rank int rank ( index_type < G > ts ); Convert a multi rank to a standard one. Parameters ts : the multi rank to convert Return value int : the corresponding standard rank See also multi_partitioning::multi_rank","title":"multi_partitioning::rank"},{"location":"api/multi_partitioning/rank/#bulkmulti_partitioningrank","text":"int rank ( index_type < G > ts ); Convert a multi rank to a standard one.","title":"bulk::multi_partitioning::rank"},{"location":"api/multi_partitioning/rank/#parameters","text":"ts : the multi rank to convert","title":"Parameters"},{"location":"api/multi_partitioning/rank/#return-value","text":"int : the corresponding standard rank","title":"Return value"},{"location":"api/multi_partitioning/rank/#see-also","text":"multi_partitioning::multi_rank","title":"See also"},{"location":"api/partitioning/constructor/","text":"bulk::partitioning::partitioning partitioning ( index_type < D > global_size ); Constructs a partitioning. Parameters global_size - the shape of the data","title":"partitioning::constructor"},{"location":"api/partitioning/constructor/#bulkpartitioningpartitioning","text":"partitioning ( index_type < D > global_size ); Constructs a partitioning.","title":"bulk::partitioning::partitioning"},{"location":"api/partitioning/constructor/#parameters","text":"global_size - the shape of the data","title":"Parameters"},{"location":"api/partitioning/deconstructor/","text":"bulk::partitioning::~partitioning virtual ~ partitioning () = default ; Virtual deconstructor.","title":"partitioning::deconstructor"},{"location":"api/partitioning/deconstructor/#bulkpartitioningpartitioning","text":"virtual ~ partitioning () = default ; Virtual deconstructor.","title":"bulk::partitioning::~partitioning"},{"location":"api/partitioning/global/","text":"bulk::partitioning::global index_type < D > global ( index_type < D > xs , int processor ); Convert a local index to a global one. Parameters processor : the local rank xs : the local index Return value index_type<D> : the global index","title":"partitioning::global"},{"location":"api/partitioning/global/#bulkpartitioningglobal","text":"index_type < D > global ( index_type < D > xs , int processor ); Convert a local index to a global one.","title":"bulk::partitioning::global"},{"location":"api/partitioning/global/#parameters","text":"processor : the local rank xs : the local index","title":"Parameters"},{"location":"api/partitioning/global/#return-value","text":"index_type<D> : the global index","title":"Return value"},{"location":"api/partitioning/global_size/","text":"bulk::partitioning::global_size index_type < D > global_size () const ; Returns the data space shape. Return value index_type<D> : The shape of the data space.","title":"partitioning::global_size"},{"location":"api/partitioning/global_size/#bulkpartitioningglobal_size","text":"index_type < D > global_size () const ; Returns the data space shape.","title":"bulk::partitioning::global_size"},{"location":"api/partitioning/global_size/#return-value","text":"index_type<D> : The shape of the data space.","title":"Return value"},{"location":"api/partitioning/local/","text":"bulk::partitioning::local index_type < D > local ( index_type < D > xs ); Convert a global index to a local one. Parameters xs : the global index Return value index_type<D> : the local index","title":"partitioning::local"},{"location":"api/partitioning/local/#bulkpartitioninglocal","text":"index_type < D > local ( index_type < D > xs ); Convert a global index to a local one.","title":"bulk::partitioning::local"},{"location":"api/partitioning/local/#parameters","text":"xs : the global index","title":"Parameters"},{"location":"api/partitioning/local/#return-value","text":"index_type<D> : the local index","title":"Return value"},{"location":"api/partitioning/local_count/","text":"bulk::partitioning::local_count size_t local_count ( int processor ); Returns the number of local elements for a given processor. Parameters processor : the local rank for which to return the shape. Return value size_t : the number of local elements","title":"partitioning::local_count"},{"location":"api/partitioning/local_count/#bulkpartitioninglocal_count","text":"size_t local_count ( int processor ); Returns the number of local elements for a given processor.","title":"bulk::partitioning::local_count"},{"location":"api/partitioning/local_count/#parameters","text":"processor : the local rank for which to return the shape.","title":"Parameters"},{"location":"api/partitioning/local_count/#return-value","text":"size_t : the number of local elements","title":"Return value"},{"location":"api/partitioning/local_size/","text":"bulk::partitioning::local_size virtual index_type < D > local_size ( int processor ); Returns the local data space shape for a given processor. Parameters processor : the local rank for which to return the shape. Return value index_type<D> : The shape of the local data space.","title":"partitioning::local_size"},{"location":"api/partitioning/local_size/#bulkpartitioninglocal_size","text":"virtual index_type < D > local_size ( int processor ); Returns the local data space shape for a given processor.","title":"bulk::partitioning::local_size"},{"location":"api/partitioning/local_size/#parameters","text":"processor : the local rank for which to return the shape.","title":"Parameters"},{"location":"api/partitioning/local_size/#return-value","text":"index_type<D> : The shape of the local data space.","title":"Return value"},{"location":"api/partitioning/owner/","text":"bulk::partitioning::owner virtual int owner ( index_type < D > xs ); Returns the rank of the processor that owns the element at a given index. Parameters xs : the index in the data space Return value int : the rank of the processor that owns xs","title":"partitioning::owner"},{"location":"api/partitioning/owner/#bulkpartitioningowner","text":"virtual int owner ( index_type < D > xs ); Returns the rank of the processor that owns the element at a given index.","title":"bulk::partitioning::owner"},{"location":"api/partitioning/owner/#parameters","text":"xs : the index in the data space","title":"Parameters"},{"location":"api/partitioning/owner/#return-value","text":"int : the rank of the processor that owns xs","title":"Return value"},{"location":"api/queue/begin/","text":"bulk::queue::begin iterator begin (); Obtain an iterator to the begin of the local queue. Return value an iterator to the begin of the local queue","title":"queue::begin"},{"location":"api/queue/begin/#bulkqueuebegin","text":"iterator begin (); Obtain an iterator to the begin of the local queue.","title":"bulk::queue::begin"},{"location":"api/queue/begin/#return-value","text":"an iterator to the begin of the local queue","title":"Return value"},{"location":"api/queue/constructor/","text":"bulk::queue::queue queue ( world & world ); Constructs a message queue for use in world . Parameters world - the world this queue belongs to","title":"queue::constructor"},{"location":"api/queue/constructor/#bulkqueuequeue","text":"queue ( world & world ); Constructs a message queue for use in world .","title":"bulk::queue::queue"},{"location":"api/queue/constructor/#parameters","text":"world - the world this queue belongs to","title":"Parameters"},{"location":"api/queue/deconstructor/","text":"bulk::queue::~queue ~ queue (); Deconstructs a queue, clears and deregisters it.","title":"queue::deconstructor"},{"location":"api/queue/deconstructor/#bulkqueuequeue","text":"~ queue (); Deconstructs a queue, clears and deregisters it.","title":"bulk::queue::~queue"},{"location":"api/queue/empty/","text":"bulk::queue::empty bool empty () const ; Checks whether the inbox is empty. Return value true if the inbox is empty, false otherwise.","title":"queue::empty"},{"location":"api/queue/empty/#bulkqueueempty","text":"bool empty () const ; Checks whether the inbox is empty.","title":"bulk::queue::empty"},{"location":"api/queue/empty/#return-value","text":"true if the inbox is empty, false otherwise.","title":"Return value"},{"location":"api/queue/end/","text":"bulk::queue::end iterator end (); Obtain an iterator to the end of the local queue. Return value an iterator to the end of the local queue","title":"queue::end"},{"location":"api/queue/end/#bulkqueueend","text":"iterator end (); Obtain an iterator to the end of the local queue.","title":"bulk::queue::end"},{"location":"api/queue/end/#return-value","text":"an iterator to the end of the local queue","title":"Return value"},{"location":"api/queue/sender/","text":"bulk::queue::sender Defined in header <bulk/messages.hpp> . class sender ; Provides a way to send messages to remote queues. Member functions Value access send assign values to the variable","title":"queue::sender"},{"location":"api/queue/sender/#bulkqueuesender","text":"Defined in header <bulk/messages.hpp> . class sender ; Provides a way to send messages to remote queues.","title":"bulk::queue::sender"},{"location":"api/queue/sender/#member-functions","text":"Value access send assign values to the variable","title":"Member functions"},{"location":"api/queue/size/","text":"bulk::queue::size size_t size () const ; Obtain the number of messages in the local queue. Return value the number of messages in the local queue","title":"queue::size"},{"location":"api/queue/size/#bulkqueuesize","text":"size_t size () const ; Obtain the number of messages in the local queue.","title":"bulk::queue::size"},{"location":"api/queue/size/#return-value","text":"the number of messages in the local queue","title":"Return value"},{"location":"api/queue/world/","text":"bulk::queue::world bulk :: world & world (); Returns a reference to the world the queue belongs to Return value A reference to the world.","title":"queue::world"},{"location":"api/queue/world/#bulkqueueworld","text":"bulk :: world & world (); Returns a reference to the world the queue belongs to","title":"bulk::queue::world"},{"location":"api/queue/world/#return-value","text":"A reference to the world.","title":"Return value"},{"location":"api/queue/sender/send/","text":"bulk::queue::sender::send template < typename ... Us > void send ( Us ... args ) Send a message to a remote queue Parameters args - the content to send Complexity and cost Cost - sizeof(Us...) * g","title":"queue::sender::send"},{"location":"api/queue/sender/send/#bulkqueuesendersend","text":"template < typename ... Us > void send ( Us ... args ) Send a message to a remote queue","title":"bulk::queue::sender::send"},{"location":"api/queue/sender/send/#parameters","text":"args - the content to send","title":"Parameters"},{"location":"api/queue/sender/send/#complexity-and-cost","text":"Cost - sizeof(Us...) * g","title":"Complexity and cost"},{"location":"api/rectangular_partitioning/constructor/","text":"bulk::rectangular_partitioning::rectangular_partitioning rectangular_partitioning ( index_type < D > global_size , index_type < G > grid_size ); Constructs a rectangular partitioning. Parameters global_size - the shape of the data grid_size - the shape of the processor grid","title":"rectangular_partitioning::constructor"},{"location":"api/rectangular_partitioning/constructor/#bulkrectangular_partitioningrectangular_partitioning","text":"rectangular_partitioning ( index_type < D > global_size , index_type < G > grid_size ); Constructs a rectangular partitioning.","title":"bulk::rectangular_partitioning::rectangular_partitioning"},{"location":"api/rectangular_partitioning/constructor/#parameters","text":"global_size - the shape of the data grid_size - the shape of the processor grid","title":"Parameters"},{"location":"api/rectangular_partitioning/deconstructor/","text":"bulk::rectangular_partitioning::~rectangular_partitioning virtual ~ rectangular_partitioning () = default ; Virtual deconstructor.","title":"rectangular_partitioning::deconstructor"},{"location":"api/rectangular_partitioning/deconstructor/#bulkrectangular_partitioningrectangular_partitioning","text":"virtual ~ rectangular_partitioning () = default ; Virtual deconstructor.","title":"bulk::rectangular_partitioning::~rectangular_partitioning"},{"location":"api/rectangular_partitioning/origin/","text":"bulk::rectangular_partitioning::origin virtual index_type < D > origin ( index_type < G > processor ) const ; // (1) virtual index_type < D > origin ( int processor ) const ; // (2) Returns the origin of the local data. Parameters processor : the multi rank (1) or rank (2) for which to return the local origin. Return value index_type<D> : The origin of the local data space.","title":"rectangular_partitioning::origin"},{"location":"api/rectangular_partitioning/origin/#bulkrectangular_partitioningorigin","text":"virtual index_type < D > origin ( index_type < G > processor ) const ; // (1) virtual index_type < D > origin ( int processor ) const ; // (2) Returns the origin of the local data.","title":"bulk::rectangular_partitioning::origin"},{"location":"api/rectangular_partitioning/origin/#parameters","text":"processor : the multi rank (1) or rank (2) for which to return the local origin.","title":"Parameters"},{"location":"api/rectangular_partitioning/origin/#return-value","text":"index_type<D> : The origin of the local data space.","title":"Return value"},{"location":"api/var/T_operator/","text":"bulk::var::operator T operator T & (); operator const T & (); Obtain an implicit (const) reference to the value of the local image. Example This allows for convenient syntax when working with local images, e.g.: auto x = bulk :: var < int > ( world , 5 ); auto y = x + 5 ; // y is an int with value 10","title":"var::operator T"},{"location":"api/var/T_operator/#bulkvaroperator-t","text":"operator T & (); operator const T & (); Obtain an implicit (const) reference to the value of the local image.","title":"bulk::var::operator T"},{"location":"api/var/T_operator/#example","text":"This allows for convenient syntax when working with local images, e.g.: auto x = bulk :: var < int > ( world , 5 ); auto y = x + 5 ; // y is an int with value 10","title":"Example"},{"location":"api/var/assignment_operator/","text":"bulk::var::operator= void operator = ( var < T >&& other ); Move assignment operator. Replaces the target variable *this with the source variable other , and invalidates the source. Parameters other - another variable to move away from Complexity and cost Cost - l or free (backend dependent)","title":"var::operator="},{"location":"api/var/assignment_operator/#bulkvaroperator","text":"void operator = ( var < T >&& other ); Move assignment operator. Replaces the target variable *this with the source variable other , and invalidates the source.","title":"bulk::var::operator="},{"location":"api/var/assignment_operator/#parameters","text":"other - another variable to move away from","title":"Parameters"},{"location":"api/var/assignment_operator/#complexity-and-cost","text":"Cost - l or free (backend dependent)","title":"Complexity and cost"},{"location":"api/var/broadcast/","text":"bulk::var::broadcast void broadcast ( T x ); Broadcasts a value to all images. For one variable, this function should be called by a single processor in any given superstep. The broadcasted value is valid starting from the subsequent superstep. Parameters x - the value to broadcast Complexity and cost Cost - p * g","title":"var::broadcast"},{"location":"api/var/broadcast/#bulkvarbroadcast","text":"void broadcast ( T x ); Broadcasts a value to all images. For one variable, this function should be called by a single processor in any given superstep. The broadcasted value is valid starting from the subsequent superstep.","title":"bulk::var::broadcast"},{"location":"api/var/broadcast/#parameters","text":"x - the value to broadcast","title":"Parameters"},{"location":"api/var/broadcast/#complexity-and-cost","text":"Cost - p * g","title":"Complexity and cost"},{"location":"api/var/constructor/","text":"bulk::var::var var ( world & world ); // (1) var ( world & world , T value ); // (2) Constructs a variable and registers it with world . Requires T to be trivially constructable. \u2026 In addition, set the initial value of the local image to value . Parameters world - the world this variable belongs to value - initial value of the local image Complexity and cost Cost - l or free (backend dependent)","title":"var::constructor"},{"location":"api/var/constructor/#bulkvarvar","text":"var ( world & world ); // (1) var ( world & world , T value ); // (2) Constructs a variable and registers it with world . Requires T to be trivially constructable. \u2026 In addition, set the initial value of the local image to value .","title":"bulk::var::var"},{"location":"api/var/constructor/#parameters","text":"world - the world this variable belongs to value - initial value of the local image","title":"Parameters"},{"location":"api/var/constructor/#complexity-and-cost","text":"Cost - l or free (backend dependent)","title":"Complexity and cost"},{"location":"api/var/deconstructor/","text":"bulk::var::~var ~ var (); Deconstructs a variable and deregisters it with its world . Complexity and cost Cost - l or free, depending on the backend","title":"var::deconstructor"},{"location":"api/var/deconstructor/#bulkvarvar","text":"~ var (); Deconstructs a variable and deregisters it with its world .","title":"bulk::var::~var"},{"location":"api/var/deconstructor/#complexity-and-cost","text":"Cost - l or free, depending on the backend","title":"Complexity and cost"},{"location":"api/var/image/","text":"bulk::var::image Defined in header <bulk/variable.hpp> . class image ; bulk::var::image provides syntactic sugar for dealing with remote images. Member functions Value access operator= assign values to the variable get returns the value of the local variable image","title":"var::image"},{"location":"api/var/image/#bulkvarimage","text":"Defined in header <bulk/variable.hpp> . class image ; bulk::var::image provides syntactic sugar for dealing with remote images.","title":"bulk::var::image"},{"location":"api/var/image/#member-functions","text":"Value access operator= assign values to the variable get returns the value of the local variable image","title":"Member functions"},{"location":"api/var/paren_operator/","text":"bulk::var::operator() image operator ()( int t ); Obtain an image object, used to communicate with a remote image. Parameters t - the id of the remote processor","title":"var::operator()"},{"location":"api/var/paren_operator/#bulkvaroperator","text":"image operator ()( int t ); Obtain an image object, used to communicate with a remote image.","title":"bulk::var::operator()"},{"location":"api/var/paren_operator/#parameters","text":"t - the id of the remote processor","title":"Parameters"},{"location":"api/var/value/","text":"bulk::var::value T & value (); Returns a reference to the value held by the local image of the variable. Return value A reference to the value of the local image","title":"var::value"},{"location":"api/var/value/#bulkvarvalue","text":"T & value (); Returns a reference to the value held by the local image of the variable.","title":"bulk::var::value"},{"location":"api/var/value/#return-value","text":"A reference to the value of the local image","title":"Return value"},{"location":"api/var/world/","text":"bulk::var::world bulk :: world & world (); Returns a reference to the world the variable belongs to Return value A reference to the world.","title":"var::world"},{"location":"api/var/world/#bulkvarworld","text":"bulk :: world & world (); Returns a reference to the world the variable belongs to","title":"bulk::var::world"},{"location":"api/var/world/#return-value","text":"A reference to the world.","title":"Return value"},{"location":"api/var/image/assignment_operator/","text":"bulk::var::image::operator= var < T >& operator = ( const T & value ); Assign a value to a remote image Parameters value - the new value of the image Complexity and cost Cost - sizeof(T) * g","title":"var::image::operator="},{"location":"api/var/image/assignment_operator/#bulkvarimageoperator","text":"var < T >& operator = ( const T & value ); Assign a value to a remote image","title":"bulk::var::image::operator="},{"location":"api/var/image/assignment_operator/#parameters","text":"value - the new value of the image","title":"Parameters"},{"location":"api/var/image/assignment_operator/#complexity-and-cost","text":"Cost - sizeof(T) * g","title":"Complexity and cost"},{"location":"api/var/image/get/","text":"bulk::var::image::get future < T > get () const ; Obtain a future to a remote image value Complexity and cost Cost - sizeof(T) * g ;","title":"var::image::get"},{"location":"api/var/image/get/#bulkvarimageget","text":"future < T > get () const ; Obtain a future to a remote image value","title":"bulk::var::image::get"},{"location":"api/var/image/get/#complexity-and-cost","text":"Cost - sizeof(T) * g ;","title":"Complexity and cost"},{"location":"api/world/active_processors/","text":"bulk::world::active_processors int active_processors () const ; Returns the total number of active processors in an SPMD section. Return value int : The number of active processors.","title":"world::active_processors"},{"location":"api/world/active_processors/#bulkworldactive_processors","text":"int active_processors () const ; Returns the total number of active processors in an SPMD section.","title":"bulk::world::active_processors"},{"location":"api/world/active_processors/#return-value","text":"int : The number of active processors.","title":"Return value"},{"location":"api/world/barrier/","text":"bulk::world::barrier void barrier () const ; Performs a global barrier of the active processors. Complexity and cost Cost - l (approximately)","title":"world::barrier"},{"location":"api/world/barrier/#bulkworldbarrier","text":"void barrier () const ; Performs a global barrier of the active processors.","title":"bulk::world::barrier"},{"location":"api/world/barrier/#complexity-and-cost","text":"Cost - l (approximately)","title":"Complexity and cost"},{"location":"api/world/log/","text":"bulk::world::log template < typename ... Ts > ; void log ( const char * format , const Ts & ... ts ); Print output for debugging, shown at the next synchronization, using printf style formatting syntax.","title":"world::log"},{"location":"api/world/log/#bulkworldlog","text":"template < typename ... Ts > ; void log ( const char * format , const Ts & ... ts ); Print output for debugging, shown at the next synchronization, using printf style formatting syntax.","title":"bulk::world::log"},{"location":"api/world/log_once/","text":"bulk::world::log_once template < typename ... Ts > ; void log_once ( const char * format , const Ts & ... ts ); Print output for debugging, shown at the next synchronization, using printf style formatting syntax. Only messages by the first processor get output.","title":"world::log_once"},{"location":"api/world/log_once/#bulkworldlog_once","text":"template < typename ... Ts > ; void log_once ( const char * format , const Ts & ... ts ); Print output for debugging, shown at the next synchronization, using printf style formatting syntax. Only messages by the first processor get output.","title":"bulk::world::log_once"},{"location":"api/world/next_rank/","text":"bulk::world::next_rank int next_rank () const ; Returns the id of the next logical processor. Return value int : The id of the next processor","title":"world::next_rank"},{"location":"api/world/next_rank/#bulkworldnext_rank","text":"int next_rank () const ; Returns the id of the next logical processor.","title":"bulk::world::next_rank"},{"location":"api/world/next_rank/#return-value","text":"int : The id of the next processor","title":"Return value"},{"location":"api/world/prev_rank/","text":"bulk::world::prev_rank int prev_rank () const ; Returns the id of the previous logical processor. Return value int : The id of the previous processor","title":"world::prev_rank"},{"location":"api/world/prev_rank/#bulkworldprev_rank","text":"int prev_rank () const ; Returns the id of the previous logical processor.","title":"bulk::world::prev_rank"},{"location":"api/world/prev_rank/#return-value","text":"int : The id of the previous processor","title":"Return value"},{"location":"api/world/rank/","text":"bulk::world::rank int rank () const ; Returns the local processor id Return value int : The id of the local processor.","title":"world::rank"},{"location":"api/world/rank/#bulkworldrank","text":"int rank () const ; Returns the local processor id","title":"bulk::world::rank"},{"location":"api/world/rank/#return-value","text":"int : The id of the local processor.","title":"Return value"},{"location":"api/world/split/","text":"bulk::world::split std :: unique_ptr < bulk :: world > split ( int part ) Split the world into a number of parts. Every processor in the same part after splitting get assigned consecutive 0-based ranks in the subworld in ascending order from their rank in the ambient world. The subworld is returned as a unique pointer, to ensure proper clean up when it is no longer needed. This allows synchronization and communication between subsets of processors. There is no limit on the number of splits, and resources get cleaned up automatically when subworlds are no longer used. Example auto sub = world . split ( s % 2 ); // Ranks get reordered according to their original ranks in 'world' assert ( sub -> rank () == world . rank () / 2 ); // Otherwise, sub acts just like world auto x = bulk :: var < int > ( * sub , s ); auto y = x ( sub -> next_rank ()). get (); sub -> sync (); // The next rank in the subworld, is the next of the next rank // in the original 'world'. assert ( y . value () == (( s + 2 ) % p ));","title":"world::split"},{"location":"api/world/split/#bulkworldsplit","text":"std :: unique_ptr < bulk :: world > split ( int part ) Split the world into a number of parts. Every processor in the same part after splitting get assigned consecutive 0-based ranks in the subworld in ascending order from their rank in the ambient world. The subworld is returned as a unique pointer, to ensure proper clean up when it is no longer needed. This allows synchronization and communication between subsets of processors. There is no limit on the number of splits, and resources get cleaned up automatically when subworlds are no longer used.","title":"bulk::world::split"},{"location":"api/world/split/#example","text":"auto sub = world . split ( s % 2 ); // Ranks get reordered according to their original ranks in 'world' assert ( sub -> rank () == world . rank () / 2 ); // Otherwise, sub acts just like world auto x = bulk :: var < int > ( * sub , s ); auto y = x ( sub -> next_rank ()). get (); sub -> sync (); // The next rank in the subworld, is the next of the next rank // in the original 'world'. assert ( y . value () == (( s + 2 ) % p ));","title":"Example"},{"location":"api/world/sync/","text":"bulk::world::sync void sync ( bool clear_queues = true ) const ; Performs a global barrier synchronization of the active processors, resolving any outstanding communication. By default, this will clear the message queues. To retain messages currently stored in queues, set clear_queues to false . Complexity and cost Cost - l","title":"world::sync"},{"location":"api/world/sync/#bulkworldsync","text":"void sync ( bool clear_queues = true ) const ; Performs a global barrier synchronization of the active processors, resolving any outstanding communication. By default, this will clear the message queues. To retain messages currently stored in queues, set clear_queues to false .","title":"bulk::world::sync"},{"location":"api/world/sync/#complexity-and-cost","text":"Cost - l","title":"Complexity and cost"},{"location":"backends/mpi/","text":"MPI If you choose to use the MPI backend, then simply include the Bulk headers, and set up your project build as though it is an MPI project. We suggest to use the OpenMPI implementation, since is the implementation we test against. FAQ On Fedora 28 using GCC 8 or later, you may receive a error -Werror=cast-function-type . This is due to OpenMPI loading the C++ bindings, which Bulk does not use. You can circumvent this issue by generating the build files with the following. cmake .. -DCMAKE_CXX_FLAGS = \"-DOMPI_SKIP_MPICXX\"","title":"MPI"},{"location":"backends/mpi/#mpi","text":"If you choose to use the MPI backend, then simply include the Bulk headers, and set up your project build as though it is an MPI project. We suggest to use the OpenMPI implementation, since is the implementation we test against.","title":"MPI"},{"location":"backends/mpi/#faq","text":"On Fedora 28 using GCC 8 or later, you may receive a error -Werror=cast-function-type . This is due to OpenMPI loading the C++ bindings, which Bulk does not use. You can circumvent this issue by generating the build files with the following. cmake .. -DCMAKE_CXX_FLAGS = \"-DOMPI_SKIP_MPICXX\"","title":"FAQ"},{"location":"backends/thread/","text":"<thread> This backend uses the shared-memory parallelism provided by the standard C++ library <thread> . To use it on Linux, link against the pthread library. There are two supported barrier implementations, a mutex-based barrier and the (default) spinlock-based barrier. The mutex barrier is more energy efficient, while the spinlock barrier is significantly faster. Choosing the barrier can be done by passing a template argument to bulk::thread::environment_ , while bulk::thread::environment is a non-template type alias to the environment using the default barrier implementation. // default (with spinning barrier) auto env = bulk :: thread :: environment ; // explicit (with spinning barrier) auto env = bulk :: thread :: environment_ < bulk :: thread :: spinning_barrier > ; // explicit (with mutex barrier) auto env = bulk :: thread :: environment_ < bulk :: thread :: mutex_barrier > ;","title":"thread"},{"location":"backends/thread/#thread","text":"This backend uses the shared-memory parallelism provided by the standard C++ library <thread> . To use it on Linux, link against the pthread library. There are two supported barrier implementations, a mutex-based barrier and the (default) spinlock-based barrier. The mutex barrier is more energy efficient, while the spinlock barrier is significantly faster. Choosing the barrier can be done by passing a template argument to bulk::thread::environment_ , while bulk::thread::environment is a non-template type alias to the environment using the default barrier implementation. // default (with spinning barrier) auto env = bulk :: thread :: environment ; // explicit (with spinning barrier) auto env = bulk :: thread :: environment_ < bulk :: thread :: spinning_barrier > ; // explicit (with mutex barrier) auto env = bulk :: thread :: environment_ < bulk :: thread :: mutex_barrier > ;","title":"&lt;thread&gt;"}]}